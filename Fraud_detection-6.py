# -*- coding: utf-8 -*-
"""
Biomass Pyrolysis Yield Forecast using CatBoost Ensemble Models
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import glob
import joblib
import json
import traceback

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title='Biomass Pyrolysis Yield Forecast',
    page_icon='ğŸ“Š',
    layout='wide'
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown(
    """
    <style>
    /* å…¨å±€å­—ä½“è®¾ç½® */
    html, body, [class*="css"] {
        font-size: 16px !important;
    }
    
    /* æ ‡é¢˜ */
    .main-title {
        text-align: center;
        font-size: 32px !important;
        font-weight: bold;
        margin-bottom: 20px;
        color: white !important;
    }
    
    /* åŒºåŸŸæ ·å¼ */
    .section-header {
        color: white;
        font-weight: bold;
        font-size: 22px;
        text-align: center;
        padding: 10px;
        border-radius: 8px;
        margin-bottom: 15px;
    }
    
    /* è¾“å…¥æ ‡ç­¾æ ·å¼ */
    .input-label {
        padding: 5px;
        border-radius: 5px;
        margin-bottom: 5px;
        font-size: 18px;
        color: white;
    }
    
    /* ç»“æœæ˜¾ç¤ºæ ·å¼ */
    .yield-result {
        background-color: #1E1E1E;
        color: white;
        font-size: 36px;
        font-weight: bold;
        text-align: center;
        padding: 15px;
        border-radius: 8px;
        margin-top: 20px;
    }
    
    /* å¼ºåˆ¶åº”ç”¨ç™½è‰²èƒŒæ™¯åˆ°è¾“å…¥æ¡† */
    [data-testid="stNumberInput"] input {
        background-color: white !important;
        color: black !important;
    }
    
    /* å¢å¤§æŒ‰é’®çš„å­—ä½“ */
    .stButton button {
        font-size: 18px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ä¸»æ ‡é¢˜
st.markdown("<h1 class='main-title'>Prediction of crop biomass pyrolysis yield based on CatBoost ensemble modeling</h1>", unsafe_allow_html=True)

# åˆ›å»ºä¾§è¾¹æ æ—¥å¿—åŒºåŸŸ
log_container = st.sidebar.container()
log_container.write("### è°ƒè¯•æ—¥å¿—")

def log(message):
    """è®°å½•è°ƒè¯•ä¿¡æ¯åˆ°ä¾§è¾¹æ """
    log_container.write(message)

# å¢åŠ æœç´¢æ¨¡å‹æ–‡ä»¶çš„åŠŸèƒ½
def find_model_files():
    """
    æœç´¢ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶å’Œæ ‡å‡†åŒ–å™¨æ–‡ä»¶
    """
    # æœç´¢å½“å‰ç›®å½•åŠå­ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶
    model_files = glob.glob("**/model_*.joblib", recursive=True)
    model_files = sorted(model_files, key=lambda x: int(x.split('model_')[1].split('.')[0]))
    scaler_files = glob.glob("**/final_scaler.joblib", recursive=True)
    metadata_files = glob.glob("**/metadata.json", recursive=True)
    
    log(f"æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
    for f in model_files:
        log(f"- {f}")
    log(f"æ‰¾åˆ° {len(scaler_files)} ä¸ªæ ‡å‡†åŒ–å™¨æ–‡ä»¶: {scaler_files}")
    log(f"æ‰¾åˆ° {len(metadata_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶: {metadata_files}")
    
    return model_files, scaler_files, metadata_files

# ä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼çš„é¢„æµ‹å™¨
class DirectPredictor:
    """ç›´æ¥åŠ è½½æ¨¡å‹æ–‡ä»¶è¿›è¡Œé¢„æµ‹çš„é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.models = []
        self.model_weights = None
        self.scaler = None
        self.metadata = None
        self.feature_names = None
        
        # æŸ¥æ‰¾å¹¶åŠ è½½æ¨¡å‹
        self.load_model_components()
    
    def load_model_components(self):
        """åŠ è½½æ¨¡å‹ç»„ä»¶"""
        try:
            # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
            model_files, scaler_files, metadata_files = find_model_files()
            
            # å…ˆåŠ è½½å…ƒæ•°æ®ï¼Œè·å–ç‰¹å¾åç§°
            if metadata_files:
                metadata_path = metadata_files[0]
                log(f"åŠ è½½å…ƒæ•°æ®: {metadata_path}")
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
                self.feature_names = self.metadata.get('feature_names', None)
                log(f"å…ƒæ•°æ®ä¸­çš„ç‰¹å¾åç§°: {self.feature_names}")
            
            # åŠ è½½æ¨¡å‹
            if model_files:
                models_dir = os.path.dirname(model_files[0])
                log(f"æ¨¡å‹ç›®å½•: {models_dir}")
                
                for model_path in model_files:
                    log(f"åŠ è½½æ¨¡å‹: {model_path}")
                    try:
                        model = joblib.load(model_path)
                        self.models.append(model)
                        log(f"æˆåŠŸåŠ è½½æ¨¡å‹ {len(self.models)}")
                    except Exception as e:
                        log(f"åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {str(e)}")
                
                # åŠ è½½æ¨¡å‹æƒé‡
                weights_path = os.path.join(models_dir, 'model_weights.npy')
                if os.path.exists(weights_path):
                    log(f"åŠ è½½æƒé‡: {weights_path}")
                    try:
                        self.model_weights = np.load(weights_path)
                        log(f"æƒé‡å½¢çŠ¶: {self.model_weights.shape}")
                    except Exception as e:
                        log(f"åŠ è½½æƒé‡å¤±è´¥: {str(e)}")
                        self.model_weights = np.ones(len(self.models)) / len(self.models)
                else:
                    log("æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨å‡ç­‰æƒé‡")
                    self.model_weights = np.ones(len(self.models)) / len(self.models)
            else:
                log("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            if scaler_files:
                scaler_path = scaler_files[0]
                log(f"åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
                try:
                    self.scaler = joblib.load(scaler_path)
                    log("æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
                    
                    # æ£€æŸ¥æ ‡å‡†åŒ–å™¨çš„ç‰¹å¾åç§°
                    if hasattr(self.scaler, 'feature_names_in_'):
                        log(f"æ ‡å‡†åŒ–å™¨ç‰¹å¾åç§°: {self.scaler.feature_names_in_}")
                        # å¦‚æœå…ƒæ•°æ®ä¸­æ²¡æœ‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨æ ‡å‡†åŒ–å™¨ä¸­çš„
                        if self.feature_names is None:
                            self.feature_names = self.scaler.feature_names_in_.tolist()
                            log(f"ä½¿ç”¨æ ‡å‡†åŒ–å™¨ä¸­çš„ç‰¹å¾åç§°: {self.feature_names}")
                except Exception as e:
                    log(f"åŠ è½½æ ‡å‡†åŒ–å™¨å¤±è´¥: {str(e)}")
            else:
                log("æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if self.feature_names is None:
                self.feature_names = ["PT(Â°C)", "RT(min)", "C(%)", "H(%)", "O(%)", "N(%)", "Ash(%)", "VM(%)", "FC(%)", "HR(â„ƒ/min)"]
                log(f"ä½¿ç”¨é»˜è®¤ç‰¹å¾åç§°: {self.feature_names}")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
            if self.models:
                log(f"æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹")
                return True
            else:
                log("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹")
                return False
                
        except Exception as e:
            log(f"åŠ è½½æ¨¡å‹ç»„ä»¶æ—¶å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return False
    
    def predict(self, X):
        """
        ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
            X: ç‰¹å¾æ•°æ®ï¼ŒDataFramæ ¼å¼
        
        è¿”å›:
            é¢„æµ‹ç»“æœæ•°ç»„
        """
        try:
            if not self.models:
                log("æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼Œæ— æ³•é¢„æµ‹")
                return np.array([33.0])  # è¿”å›é»˜è®¤å€¼
            
            # æå–ç‰¹å¾é¡ºåº
            if isinstance(X, pd.DataFrame):
                log(f"è¾“å…¥ç‰¹å¾é¡ºåº: {X.columns.tolist()}")
                log(f"æ¨¡å‹ç‰¹å¾é¡ºåº: {self.feature_names}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ’åº
                if sorted(X.columns.tolist()) == sorted(self.feature_names):
                    log("ç‰¹å¾é›†åˆåŒ¹é…ï¼Œç¡®ä¿é¡ºåºä¸€è‡´")
                    X_ordered = X[self.feature_names].copy()
                    log(f"é‡æ’åçš„ç‰¹å¾é¡ºåº: {X_ordered.columns.tolist()}")
                else:
                    log(f"ç‰¹å¾ä¸åŒ¹é…! è¾“å…¥: {X.columns.tolist()}, æ¨¡å‹éœ€è¦: {self.feature_names}")
                    
                    # å°è¯•æ˜ å°„ç‰¹å¾
                    matching_features = {}
                    for model_feat in self.feature_names:
                        model_base = model_feat.split('(')[0]
                        for input_feat in X.columns:
                            input_base = input_feat.split('(')[0]
                            if model_base == input_base:
                                matching_features[model_feat] = input_feat
                                break
                    
                    log(f"ç‰¹å¾æ˜ å°„: {matching_features}")
                    
                    if len(matching_features) == len(self.feature_names):
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameï¼ŒæŒ‰ç…§æ¨¡å‹éœ€è¦çš„é¡ºåºå’Œåç§°
                        X_ordered = pd.DataFrame(index=X.index)
                        for model_feat in self.feature_names:
                            if model_feat in matching_features:
                                input_feat = matching_features[model_feat]
                                X_ordered[model_feat] = X[input_feat].values
                            else:
                                log(f"æ— æ³•æ˜ å°„ç‰¹å¾: {model_feat}")
                                return np.array([33.0])
                        
                        log(f"æ˜ å°„åç‰¹å¾é¡ºåº: {X_ordered.columns.tolist()}")
                    else:
                        log("æ— æ³•å®Œå…¨æ˜ å°„ç‰¹å¾åç§°")
                        return np.array([33.0])
            else:
                log("è¾“å…¥ä¸æ˜¯DataFrameæ ¼å¼")
                return np.array([33.0])
            
            # åº”ç”¨æ ‡å‡†åŒ–
            if self.scaler:
                log("åº”ç”¨æ ‡å‡†åŒ–å™¨")
                # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå»é™¤ç‰¹å¾åä»¥é¿å…é¡ºåºé—®é¢˜
                X_values = X_ordered.values
                X_scaled = self.scaler.transform(X_values)
                log(f"æ ‡å‡†åŒ–åæ•°æ®å½¢çŠ¶: {X_scaled.shape}")
            else:
                log("æ²¡æœ‰æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                X_scaled = X_ordered.values
            
            # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            all_predictions = np.zeros((X_scaled.shape[0], len(self.models)))
            
            for i, model in enumerate(self.models):
                try:
                    pred = model.predict(X_scaled)
                    all_predictions[:, i] = pred
                    log(f"æ¨¡å‹ {i} é¢„æµ‹ç»“æœ: {pred[0]:.2f}")
                except Exception as e:
                    log(f"æ¨¡å‹ {i} é¢„æµ‹å¤±è´¥: {str(e)}")
                    # ä½¿ç”¨å¹³å‡å€¼å¡«å……
                    if i > 0:
                        all_predictions[:, i] = np.mean(all_predictions[:, :i], axis=1)
            
            # è®¡ç®—åŠ æƒå¹³å‡é¢„æµ‹
            weighted_pred = np.sum(all_predictions * self.model_weights.reshape(1, -1), axis=1)
            log(f"æœ€ç»ˆåŠ æƒé¢„æµ‹ç»“æœ: {weighted_pred[0]:.2f}")
            
            return weighted_pred
            
        except Exception as e:
            log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return np.array([33.0])  # è¿”å›é»˜è®¤å€¼
    

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = DirectPredictor()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'clear_pressed' not in st.session_state:
    st.session_state.clear_pressed = False

# å®šä¹‰é»˜è®¤å€¼å’ŒèŒƒå›´
default_values = {
    "PT(Â°C)": 500.0,
    "RT(min)": 20.0,
    "C(%)": 45.0,
    "H(%)": 6.0,
    "O(%)": 40.0,
    "N(%)": 0.5,
    "Ash(%)": 5.0,
    "VM(%)": 75.0,
    "FC(%)": 15.0,
    "HR(â„ƒ/min)": 20.0
}

# ç‰¹å¾åˆ†ç±»
feature_categories = {
    "Pyrolysis Conditions": ["PT(Â°C)", "RT(min)", "HR(â„ƒ/min)"],
    "Ultimate Analysis": ["C(%)", "H(%)", "O(%)", "N(%)"],
    "Proximate Analysis": ["Ash(%)", "VM(%)", "FC(%)"]
}

# ç‰¹å¾èŒƒå›´
feature_ranges = {
    "PT(Â°C)": (300.0, 900.0),
    "RT(min)": (5.0, 120.0),
    "C(%)": (30.0, 80.0),
    "H(%)": (3.0, 10.0),
    "O(%)": (10.0, 60.0),
    "N(%)": (0.0, 5.0),
    "Ash(%)": (0.0, 25.0),
    "VM(%)": (40.0, 95.0),
    "FC(%)": (5.0, 40.0),
    "HR(â„ƒ/min)": (5.0, 100.0)
}

# åˆ›å»ºä¸‰åˆ—å¸ƒå±€
col1, col2, col3 = st.columns(3)

# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰è¾“å…¥å€¼
features = {}

# Pyrolysis Conditions (æ©™è‰²åŒºåŸŸ)
with col1:
    st.markdown("<div class='section-header' style='background-color: #FF7F50;'>Pyrolysis Conditions</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Pyrolysis Conditions"]:
        # é‡ç½®å€¼æˆ–ä½¿ç”¨ç°æœ‰å€¼
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"pyrolysis_{feature}", default_values[feature])
        
        # è·å–è¯¥ç‰¹å¾çš„èŒƒå›´
        min_val, max_val = feature_ranges[feature]
        
        # ç®€å•çš„ä¸¤åˆ—å¸ƒå±€
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #FF7F50;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"pyrolysis_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Ultimate Analysis (é»„è‰²åŒºåŸŸ)
with col2:
    st.markdown("<div class='section-header' style='background-color: #DAA520;'>Ultimate Analysis</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Ultimate Analysis"]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"ultimate_{feature}", default_values[feature])
        
        min_val, max_val = feature_ranges[feature]
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #DAA520;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"ultimate_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Proximate Analysis (ç»¿è‰²åŒºåŸŸ)
with col3:
    st.markdown("<div class='section-header' style='background-color: #32CD32;'>Proximate Analysis</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Proximate Analysis"]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"proximate_{feature}", default_values[feature])
        
        min_val, max_val = feature_ranges[feature]
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #32CD32;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"proximate_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# é‡ç½®session_stateä¸­çš„clear_pressedçŠ¶æ€
if st.session_state.clear_pressed:
    st.session_state.clear_pressed = False

# è½¬æ¢ä¸ºDataFrame
input_data = pd.DataFrame([features])

# é¢„æµ‹ç»“æœæ˜¾ç¤ºåŒºåŸŸå’ŒæŒ‰é’®
result_col, button_col = st.columns([3, 1])

with result_col:
    prediction_placeholder = st.empty()
    
with button_col:
    predict_button = st.button("PUSH", key="predict")
    
    # å®šä¹‰ClearæŒ‰é’®çš„å›è°ƒå‡½æ•°
    def clear_values():
        st.session_state.clear_pressed = True
        # æ¸…é™¤æ˜¾ç¤º
        if 'prediction_result' in st.session_state:
            st.session_state.prediction_result = None
    
    clear_button = st.button("CLEAR", key="clear", on_click=clear_values)

# å¤„ç†é¢„æµ‹é€»è¾‘
if predict_button:
    try:
        # è®°å½•è¾“å…¥æ•°æ®
        log("è¿›è¡Œé¢„æµ‹:")
        log(f"è¾“å…¥æ•°æ®: {input_data.to_dict('records')}")
        
        # ä½¿ç”¨predictorè¿›è¡Œé¢„æµ‹
        y_pred = predictor.predict(input_data)[0]
        log(f"é¢„æµ‹å®Œæˆ: {y_pred:.2f}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœåˆ°session_state
        st.session_state.prediction_result = y_pred

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        prediction_placeholder.markdown(
            f"<div class='yield-result'>Char Yield (wt%) <br> {y_pred:.2f}</div>",
            unsafe_allow_html=True
        )
    except Exception as e:
        log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        log(traceback.format_exc())
        st.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

# å¦‚æœæœ‰ä¿å­˜çš„é¢„æµ‹ç»“æœï¼Œæ˜¾ç¤ºå®ƒ
if 'prediction_result' in st.session_state and st.session_state.prediction_result is not None:
    prediction_placeholder.markdown(
        f"<div class='yield-result'>Char Yield (wt%) <br> {st.session_state.prediction_result:.2f}</div>",
        unsafe_allow_html=True
    )

# æ·»åŠ è°ƒè¯•ä¿¡æ¯
with st.expander("Debug Information", expanded=False):
    st.write("Input Features:")
    st.write(input_data)
    
    if hasattr(predictor, 'feature_names'):
        st.write("Model Features:")
        st.write(predictor.feature_names)

# æ·»åŠ å…³äºæ¨¡å‹çš„ä¿¡æ¯
st.markdown("""
### About the Model
This application uses a CatBoost ensemble model to predict char yield in biomass pyrolysis.

#### Key Factors Affecting Char Yield:
- **Pyrolysis Temperature**: Higher temperature generally decreases char yield
- **Residence Time**: Longer residence time generally increases char yield
- **Biomass Composition**: Carbon content and ash content significantly affect the final yield

The model was trained using 10-fold cross-validation with optimized hyperparameters, achieving high prediction accuracy.
""")