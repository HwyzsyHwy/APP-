# -*- coding: utf-8 -*-
"""
Biomass Pyrolysis Yield Forecast using CatBoost Ensemble Models
ä¿®å¤ç‰ˆæœ¬ - è§£å†³å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import glob
import joblib
import json
import traceback
import matplotlib.pyplot as plt
from datetime import datetime
import io
from PIL import Image

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title='Biomass Pyrolysis Yield Prediction',
    page_icon='ğŸ”¥',
    layout='wide',
    initial_sidebar_state='expanded'
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown(
    """
    <style>
    /* å…¨å±€å­—ä½“è®¾ç½® */
    html, body, [class*="css"] {
        font-size: 16px !important;
    }
    
    /* æ ‡é¢˜ */
    .main-title {
        text-align: center;
        font-size: 32px !important;
        font-weight: bold;
        margin-bottom: 20px;
        color: white !important;
    }
    
    /* åŒºåŸŸæ ·å¼ */
    .section-header {
        color: white;
        font-weight: bold;
        font-size: 22px;
        text-align: center;
        padding: 10px;
        border-radius: 8px;
        margin-bottom: 15px;
    }
    
    /* è¾“å…¥æ ‡ç­¾æ ·å¼ */
    .input-label {
        padding: 5px;
        border-radius: 5px;
        margin-bottom: 5px;
        font-size: 18px;
        color: white;
    }
    
    /* ç»“æœæ˜¾ç¤ºæ ·å¼ */
    .yield-result {
        background-color: #1E1E1E;
        color: white;
        font-size: 36px;
        font-weight: bold;
        text-align: center;
        padding: 15px;
        border-radius: 8px;
        margin-top: 20px;
    }
    
    /* å¼ºåˆ¶åº”ç”¨ç™½è‰²èƒŒæ™¯åˆ°è¾“å…¥æ¡† */
    [data-testid="stNumberInput"] input {
        background-color: white !important;
        color: black !important;
    }
    
    /* å¢å¤§æŒ‰é’®çš„å­—ä½“ */
    .stButton button {
        font-size: 18px !important;
    }
    
    /* è­¦å‘Šæ ·å¼ */
    .warning-box {
        background-color: rgba(255, 165, 0, 0.2);
        border-left: 5px solid orange;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* é”™è¯¯æ ·å¼ */
    .error-box {
        background-color: rgba(255, 0, 0, 0.2);
        border-left: 5px solid red;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* æˆåŠŸæ ·å¼ */
    .success-box {
        background-color: rgba(0, 128, 0, 0.2);
        border-left: 5px solid green;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* æ—¥å¿—æ ·å¼ */
    .log-container {
        height: 300px;
        overflow-y: auto;
        background-color: #1E1E1E;
        color: #00FF00;
        font-family: 'Courier New', monospace;
        padding: 10px;
        border-radius: 5px;
        font-size: 14px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# åˆ›å»ºä¾§è¾¹æ æ—¥å¿—åŒºåŸŸ
log_container = st.sidebar.container()
log_container.markdown("<h3>æ‰§è¡Œæ—¥å¿—</h3>", unsafe_allow_html=True)
log_text = st.sidebar.empty()

# åˆå§‹åŒ–æ—¥å¿—å­—ç¬¦ä¸²
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []

def log(message):
    """è®°å½•æ—¥å¿—åˆ°ä¾§è¾¹æ å’Œä¼šè¯çŠ¶æ€"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    st.session_state.log_messages.append(log_entry)
    # åªä¿ç•™æœ€è¿‘çš„100æ¡æ—¥å¿—
    if len(st.session_state.log_messages) > 100:
        st.session_state.log_messages = st.session_state.log_messages[-100:]
    
    # æ›´æ–°æ—¥å¿—æ˜¾ç¤º
    log_text.markdown(
        f"<div class='log-container'>{'<br>'.join(st.session_state.log_messages)}</div>", 
        unsafe_allow_html=True
    )

# ä¸»æ ‡é¢˜
st.markdown("<h1 class='main-title'>Prediction of crop biomass pyrolysis yield based on CatBoost ensemble modeling</h1>", unsafe_allow_html=True)

class CorrectedEnsemblePredictor:
    """ä¿®å¤ç‰ˆé›†æˆæ¨¡å‹é¢„æµ‹å™¨ - è§£å†³å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜"""
    
    def __init__(self):
        self.models = []
        self.scalers = []  # æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨
        self.final_scaler = None  # æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼ˆå¤‡ç”¨ï¼‰
        self.model_weights = None
        self.feature_names = None
        self.target_name = "Char Yield(%)"
        self.metadata = None
        self.model_dir = None
        self.feature_importance = None
        self.training_ranges = {}
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    def find_model_directory(self):
        """æŸ¥æ‰¾æ¨¡å‹ç›®å½•çš„å¤šç§æ–¹æ³•"""
        # æ¨¡å‹ç›®å½•å¯èƒ½çš„è·¯å¾„
        possible_dirs = [
            # ç›´æ¥è·¯å¾„
            "Char_Yield_Model",
            "Char_Yield%_Model",
            # ç›¸å¯¹è·¯å¾„
            "./Char_Yield_Model",
            "./Char_Yield%_Model",
            "../Char_Yield_Model",
            "../Char_Yield%_Model",
            # ç»å¯¹è·¯å¾„ç¤ºä¾‹
            "C:/Users/HWY/Desktop/æ–¹-3/Char_Yield_Model",
            "C:/Users/HWY/Desktop/æ–¹-3/Char_Yield%_Model"
        ]
        
        # å°è¯•æ‰€æœ‰å¯èƒ½è·¯å¾„
        for dir_path in possible_dirs:
            if os.path.exists(dir_path) and os.path.isdir(dir_path):
                log(f"æ‰¾åˆ°æ¨¡å‹ç›®å½•: {dir_path}")
                return os.path.abspath(dir_path)
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡æ¨¡å‹æ–‡ä»¶æ¨æ–­
        try:
            model_files = glob.glob("**/model_*.joblib", recursive=True)
            if model_files:
                model_dir = os.path.dirname(os.path.dirname(model_files[0]))
                log(f"åŸºäºæ¨¡å‹æ–‡ä»¶æ¨æ–­æ¨¡å‹ç›®å½•: {model_dir}")
                return model_dir
        except Exception as e:
            log(f"é€šè¿‡æ¨¡å‹æ–‡ä»¶æ¨æ–­ç›®å½•æ—¶å‡ºé”™: {str(e)}")
        
        # å½“å‰ç›®å½•ä½œä¸ºæœ€åçš„é€€è·¯
        log("è­¦å‘Š: æ— æ³•æ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œå°†ä½¿ç”¨å½“å‰ç›®å½•")
        return os.getcwd()
    
    def load_feature_importance(self):
        """åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®"""
        try:
            # å°è¯•ä»CSVæ–‡ä»¶åŠ è½½ç‰¹å¾é‡è¦æ€§
            importance_csv = os.path.join(self.model_dir, "feature_importance.csv")
            if os.path.exists(importance_csv):
                importance_df = pd.read_csv(importance_csv)
                self.feature_importance = importance_df
                log(f"å·²åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®ï¼Œå…± {len(importance_df)} ä¸ªç‰¹å¾")
                return True
            
            # å¦‚æœCSVä¸å­˜åœ¨ï¼Œå°è¯•ä»å…ƒæ•°æ®ä¸­åŠ è½½
            if self.metadata and 'feature_importance' in self.metadata:
                importance_data = self.metadata['feature_importance']
                self.feature_importance = pd.DataFrame(importance_data)
                log(f"ä»å…ƒæ•°æ®åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®")
                return True
            
            # å°è¯•é€šè¿‡åŠ è½½çš„æ¨¡å‹è®¡ç®—ç‰¹å¾é‡è¦æ€§
            if self.models and self.model_weights is not None and self.feature_names:
                log("é€šè¿‡æ¨¡å‹è®¡ç®—ç‰¹å¾é‡è¦æ€§")
                importance = np.zeros(len(self.feature_names))
                for i, model in enumerate(self.models):
                    try:
                        model_importance = model.get_feature_importance()
                        importance += model_importance * self.model_weights[i]
                    except Exception as e:
                        log(f"è·å–æ¨¡å‹ {i} ç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {str(e)}")
                
                self.feature_importance = pd.DataFrame({
                    'Feature': self.feature_names,
                    'Importance': importance
                }).sort_values('Importance', ascending=False)
                
                log(f"è®¡ç®—å¾—åˆ°ç‰¹å¾é‡è¦æ€§æ•°æ®ï¼Œæœ€é‡è¦ç‰¹å¾: {self.feature_importance['Feature'].iloc[0]}")
                return True
                
            log("è­¦å‘Š: æ— æ³•åŠ è½½æˆ–è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            return False
        except Exception as e:
            log(f"åŠ è½½ç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {str(e)}")
            return False
    
    def extract_training_ranges(self):
        """ä»æ ‡å‡†åŒ–å™¨ä¸­æå–è®­ç»ƒæ•°æ®èŒƒå›´"""
        if not hasattr(self.final_scaler, 'mean_') or not hasattr(self.final_scaler, 'scale_'):
            log("è­¦å‘Š: æ ‡å‡†åŒ–å™¨æ²¡æœ‰å‡å€¼æˆ–æ ‡å‡†å·®ä¿¡æ¯")
            return
        
        if not self.feature_names:
            log("è­¦å‘Š: æ— æ³•è·å–ç‰¹å¾åç§°")
            return
        
        # æå–ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
        means = self.final_scaler.mean_
        stds = self.final_scaler.scale_
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„95%ç½®ä¿¡åŒºé—´ (å‡å€¼Â±2æ ‡å‡†å·®)
        for i, feature in enumerate(self.feature_names):
            if i < len(means) and i < len(stds):
                mean_val = means[i]
                std_val = stds[i]
                self.training_ranges[feature] = {
                    'mean': mean_val,
                    'std': std_val,
                    'min': mean_val - 2 * std_val,  # è¿‘ä¼¼95%ç½®ä¿¡åŒºé—´ä¸‹é™
                    'max': mean_val + 2 * std_val,  # è¿‘ä¼¼95%ç½®ä¿¡åŒºé—´ä¸Šé™
                }
        
        if self.training_ranges:
            log(f"å·²æå– {len(self.training_ranges)} ä¸ªç‰¹å¾çš„è®­ç»ƒèŒƒå›´")
    
    def load_model(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶ï¼ŒåŒ…æ‹¬æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨"""
        try:
            # 1. æŸ¥æ‰¾æ¨¡å‹ç›®å½•
            self.model_dir = self.find_model_directory()
            log(f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {self.model_dir}")
            
            # 2. åŠ è½½å…ƒæ•°æ®
            metadata_path = os.path.join(self.model_dir, 'metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
                
                # è·å–ç‰¹å¾åç§°å’Œç›®æ ‡å˜é‡
                self.feature_names = self.metadata.get('feature_names', None)
                if self.metadata.get('target_name'):
                    self.target_name = self.metadata['target_name']
                
                log(f"ä»å…ƒæ•°æ®åŠ è½½ç‰¹å¾åˆ—è¡¨: {self.feature_names}")
                log(f"ç›®æ ‡å˜é‡: {self.target_name}")
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ {metadata_path}")
                # ä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨ - å¿…é¡»ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
                self.feature_names = [
                    'C(%)', 'H(%)', 'O(%)', 'N(%)', 'Ash(%)', 'VM(%)', 'FC(%)', 
                    'PT(Â°C)', 'HR(â„ƒ/min)', 'RT(min)'
                ]
                log(f"ä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨: {self.feature_names}")
            
            # 3. åŠ è½½æ¨¡å‹
            models_dir = os.path.join(self.model_dir, 'models')
            if os.path.exists(models_dir):
                model_files = sorted(glob.glob(os.path.join(models_dir, 'model_*.joblib')))
                if model_files:
                    for model_file in model_files:
                        model = joblib.load(model_file)
                        self.models.append(model)
                        log(f"åŠ è½½æ¨¡å‹: {os.path.basename(model_file)}")
                else:
                    log(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶åœ¨ {models_dir}")
                    return False
            else:
                log(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
                return False
            
            # 4. åŠ è½½æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨ - è¿™æ˜¯å…³é”®ä¿®å¤ç‚¹
            scalers_dir = os.path.join(self.model_dir, 'scalers')
            if os.path.exists(scalers_dir):
                scaler_files = sorted(glob.glob(os.path.join(scalers_dir, 'scaler_*.joblib')))
                if scaler_files:
                    for scaler_file in scaler_files:
                        scaler = joblib.load(scaler_file)
                        self.scalers.append(scaler)
                        log(f"åŠ è½½å­æ¨¡å‹æ ‡å‡†åŒ–å™¨: {os.path.basename(scaler_file)}")
                else:
                    log(f"è­¦å‘Š: æœªæ‰¾åˆ°å­æ¨¡å‹æ ‡å‡†åŒ–å™¨æ–‡ä»¶åœ¨ {scalers_dir}")
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨ç›®å½•: {scalers_dir}")
            
            # 5. åŠ è½½æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            final_scaler_path = os.path.join(self.model_dir, 'final_scaler.joblib')
            if os.path.exists(final_scaler_path):
                self.final_scaler = joblib.load(final_scaler_path)
                log(f"åŠ è½½æœ€ç»ˆæ ‡å‡†åŒ–å™¨: {final_scaler_path}")
                
                # æ‰“å°æ ‡å‡†åŒ–å™¨ä¿¡æ¯
                if hasattr(self.final_scaler, 'mean_'):
                    log(f"ç‰¹å¾å‡å€¼: {self.final_scaler.mean_}")
                if hasattr(self.final_scaler, 'scale_'):
                    log(f"ç‰¹å¾æ ‡å‡†å·®: {self.final_scaler.scale_}")
                
                # æå–è®­ç»ƒæ•°æ®èŒƒå›´
                self.extract_training_ranges()
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ€ç»ˆæ ‡å‡†åŒ–å™¨æ–‡ä»¶ {final_scaler_path}")
            
            # 6. åŠ è½½æƒé‡
            weights_path = os.path.join(self.model_dir, 'model_weights.npy')
            if os.path.exists(weights_path):
                self.model_weights = np.load(weights_path)
                log(f"åŠ è½½æƒé‡æ–‡ä»¶: {weights_path}")
            else:
                self.model_weights = np.ones(len(self.models)) / len(self.models)
                log("è­¦å‘Š: æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨å‡ç­‰æƒé‡")
            
            # 7. åŠ è½½ç‰¹å¾é‡è¦æ€§
            self.load_feature_importance()
            
            # éªŒè¯åŠ è½½çŠ¶æ€
            log(f"æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹å’Œ {len(self.scalers)} ä¸ªå­æ¨¡å‹æ ‡å‡†åŒ–å™¨")
            
            # ç‰¹åˆ«æ ‡è®°æ ‡å‡†åŒ–å™¨é—®é¢˜
            if len(self.models) != len(self.scalers):
                log(f"è­¦å‘Š: æ¨¡å‹æ•°é‡ ({len(self.models)}) ä¸æ ‡å‡†åŒ–å™¨æ•°é‡ ({len(self.scalers)}) ä¸åŒ¹é…")
                
            return True
            
        except Exception as e:
            log(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return False
    
    def check_input_range(self, input_df):
        """æ£€æŸ¥è¾“å…¥å€¼æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®èŒƒå›´å†…"""
        warnings = []
        
        if not self.training_ranges:
            log("è­¦å‘Š: æ²¡æœ‰è®­ç»ƒæ•°æ®èŒƒå›´ä¿¡æ¯ï¼Œè·³è¿‡èŒƒå›´æ£€æŸ¥")
            return warnings
        
        for feature, range_info in self.training_ranges.items():
            if feature in input_df.columns:
                value = input_df[feature].iloc[0]
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºè®­ç»ƒæ•°æ®çš„95%ç½®ä¿¡åŒºé—´
                if value < range_info['min'] or value > range_info['max']:
                    warning = f"{feature}: {value:.1f} (è¶…å‡ºè®­ç»ƒèŒƒå›´ {range_info['min']:.1f} - {range_info['max']:.1f})"
                    warnings.append(warning)
                    log(f"è­¦å‘Š: {warning}")
        
        return warnings
    
    def predict(self, input_features, return_individual=False):
        """ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨è¿›è¡Œé¢„æµ‹"""
        try:
            # éªŒè¯æ¨¡å‹ç»„ä»¶
            if not self.models or len(self.models) == 0:
                log("é”™è¯¯: æ²¡æœ‰åŠ è½½æ¨¡å‹")
                return np.array([0.0])
            
            # ç¡®ä¿è¾“å…¥ç‰¹å¾åŒ…å«æ‰€æœ‰å¿…è¦ç‰¹å¾
            missing_features = []
            if self.feature_names:
                for feature in self.feature_names:
                    if feature not in input_features.columns:
                        missing_features.append(feature)
            
            if missing_features:
                missing_str = ", ".join(missing_features)
                log(f"é”™è¯¯: è¾“å…¥ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾: {missing_str}")
                return np.array([0.0])
            
            # æŒ‰ç…§æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºé‡æ–°æ’åˆ—
            if self.feature_names:
                input_ordered = input_features[self.feature_names].copy()
                log("è¾“å…¥ç‰¹å¾å·²æŒ‰ç…§è®­ç»ƒæ—¶çš„é¡ºåºæ’åˆ—")
            else:
                input_ordered = input_features
                log("è­¦å‘Š: æ²¡æœ‰ç‰¹å¾åç§°åˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥é¡ºåº")
            
            # è®°å½•è¾“å…¥æ•°æ®
            log(f"é¢„æµ‹è¾“å…¥æ•°æ®: {input_ordered.iloc[0].to_dict()}")
            
            # ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å’Œå¯¹åº”çš„æ ‡å‡†åŒ–å™¨è¿›è¡Œé¢„æµ‹
            individual_predictions = []
            all_predictions = np.zeros((input_ordered.shape[0], len(self.models)))
            
            # æ£€æŸ¥æ ‡å‡†åŒ–å™¨æ˜¯å¦è¶³å¤Ÿ
            scalers_available = len(self.scalers) > 0
            
            for i, model in enumerate(self.models):
                try:
                    # ä½¿ç”¨å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if scalers_available and i < len(self.scalers):
                        X_scaled = self.scalers[i].transform(input_ordered)
                        log(f"æ¨¡å‹ {i} ä½¿ç”¨å¯¹åº”çš„æ ‡å‡†åŒ–å™¨")
                    else:
                        # å¦‚æœæ²¡æœ‰å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨
                        if self.final_scaler:
                            X_scaled = self.final_scaler.transform(input_ordered)
                            log(f"æ¨¡å‹ {i} ä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨")
                        else:
                            log(f"é”™è¯¯: æ¨¡å‹ {i} æ²¡æœ‰å¯ç”¨çš„æ ‡å‡†åŒ–å™¨")
                            continue
                    
                    pred = model.predict(X_scaled)
                    all_predictions[:, i] = pred
                    individual_predictions.append(float(pred[0]))
                    log(f"æ¨¡å‹ {i} é¢„æµ‹ç»“æœ: {pred[0]:.2f}")
                except Exception as e:
                    log(f"æ¨¡å‹ {i} é¢„æµ‹æ—¶å‡ºé”™: {str(e)}")
                    # å¦‚æœæŸä¸ªæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨å…¶ä»–æ¨¡å‹çš„å¹³å‡å€¼
                    if i > 0:
                        avg_pred = np.mean(all_predictions[:, :i], axis=1)
                        all_predictions[:, i] = avg_pred
                        individual_predictions.append(float(avg_pred[0]))
                        log(f"æ¨¡å‹ {i} ä½¿ç”¨ä¹‹å‰æ¨¡å‹çš„å¹³å‡å€¼: {avg_pred[0]:.2f}")
            
            # è®¡ç®—åŠ æƒå¹³å‡
            weighted_pred = np.sum(all_predictions * self.model_weights.reshape(1, -1), axis=1)
            log(f"æœ€ç»ˆåŠ æƒé¢„æµ‹ç»“æœ: {weighted_pred[0]:.2f}")
            
            if return_individual:
                return weighted_pred, individual_predictions
            else:
                return weighted_pred
            
        except Exception as e:
            log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return np.array([0.0])
    
    def get_feature_importance_plot(self):
        """ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾"""
        if self.feature_importance is None or len(self.feature_importance) == 0:
            return None
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(8, 5))
            
            # æå–æ•°æ®
            importance_df = self.feature_importance.sort_values('Importance', ascending=True)
            features = importance_df['Feature'].tolist()
            importance = importance_df['Importance'].tolist()
            
            # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
            ax.barh(features, importance, color='skyblue')
            
            # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title('Feature Importance', fontsize=14)
            ax.set_xlabel('Importance Score', fontsize=12)
            ax.set_ylabel('Feature', fontsize=12)
            
            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()
            
            # å°†å›¾è¡¨è½¬æ¢ä¸ºå›¾åƒ
            buf = io.BytesIO()
            fig.savefig(buf, format='png', dpi=100)
            buf.seek(0)
            
            # ä½¿ç”¨PILæ‰“å¼€å›¾åƒå¹¶è¿”å›
            img = Image.open(buf)
            return img
            
        except Exception as e:
            log(f"åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯æ‘˜è¦"""
        info = {
            "æ¨¡å‹ç±»å‹": "CatBoosté›†æˆæ¨¡å‹",
            "æ¨¡å‹æ•°é‡": len(self.models),
            "ç‰¹å¾æ•°é‡": len(self.feature_names) if self.feature_names else 0,
            "ç›®æ ‡å˜é‡": self.target_name
        }
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        if self.metadata and 'performance' in self.metadata:
            performance = self.metadata['performance']
            info["æµ‹è¯•é›†RÂ²"] = f"{performance.get('test_r2', 'N/A'):.4f}"
            info["æµ‹è¯•é›†RMSE"] = f"{performance.get('test_rmse', 'N/A'):.2f}"
        
        # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
        if self.feature_importance is not None and len(self.feature_importance) > 0:
            top_features = self.feature_importance.head(3)
            info["é‡è¦ç‰¹å¾"] = ", ".join(top_features['Feature'].tolist())
        
        # æ·»åŠ æ ‡å‡†åŒ–å™¨ä¿¡æ¯
        info["å­æ¨¡å‹æ ‡å‡†åŒ–å™¨æ•°é‡"] = len(self.scalers)
        
        return info

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = CorrectedEnsemblePredictor()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'clear_pressed' not in st.session_state:
    st.session_state.clear_pressed = False
if 'prediction_result' not in st.session_state:
    st.session_state.prediction_result = None
if 'warnings' not in st.session_state:
    st.session_state.warnings = []
if 'individual_predictions' not in st.session_state:
    st.session_state.individual_predictions = []

# å®šä¹‰é»˜è®¤å€¼ - ä»ç”¨æˆ·æˆªå›¾ä¸­æå–
default_values = {
    "C(%)": 38.3,
    "H(%)": 5.5,
    "O(%)": 55.2,
    "N(%)": 0.6,
    "Ash(%)": 6.6,
    "VM(%)": 81.1,
    "FC(%)": 10.3,
    "PT(Â°C)": 500.0,  # ä½¿ç”¨æ›´åˆç†çš„é»˜è®¤æ¸©åº¦
    "HR(â„ƒ/min)": 10.0,
    "RT(min)": 60.0
}

# ç‰¹å¾åˆ†ç±»
feature_categories = {
    "Ultimate Analysis": ["C(%)", "H(%)", "O(%)", "N(%)"],
    "Proximate Analysis": ["Ash(%)", "VM(%)", "FC(%)"],
    "Pyrolysis Conditions": ["PT(Â°C)", "HR(â„ƒ/min)", "RT(min)"]
}

# é¢œè‰²é…ç½®
category_colors = {
    "Ultimate Analysis": "#DAA520",  # é»„è‰²
    "Proximate Analysis": "#32CD32",  # ç»¿è‰²
    "Pyrolysis Conditions": "#FF7F50"  # æ©™è‰²
}

# åˆ›å»ºä¸‰åˆ—å¸ƒå±€
col1, col2, col3 = st.columns(3)

# ä½¿ç”¨å­—å…¸å­˜å‚¨æ‰€æœ‰è¾“å…¥å€¼
features = {}

# Ultimate Analysis - ç¬¬ä¸€åˆ—
with col1:
    category = "Ultimate Analysis"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=0.0, 
                max_value=100.0, 
                value=value, 
                key=f"{category}_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Proximate Analysis - ç¬¬äºŒåˆ—
with col2:
    category = "Proximate Analysis"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=0.0, 
                max_value=100.0, 
                value=value, 
                key=f"{category}_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Pyrolysis Conditions - ç¬¬ä¸‰åˆ—
with col3:
    category = "Pyrolysis Conditions"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        # æ ¹æ®ç‰¹å¾è®¾ç½®èŒƒå›´
        if feature == "PT(Â°C)":
            min_val, max_val = 200.0, 900.0
        elif feature == "HR(â„ƒ/min)":
            min_val, max_val = 1.0, 100.0
        elif feature == "RT(min)":
            min_val, max_val = 0.0, 120.0
        else:
            min_val, max_val = 0.0, 100.0
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"{category}_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# é‡ç½®çŠ¶æ€
if st.session_state.clear_pressed:
    st.session_state.clear_pressed = False

# é¢„æµ‹ç»“æœæ˜¾ç¤ºåŒºåŸŸ
result_container = st.container()

# é¢„æµ‹æŒ‰é’®åŒºåŸŸ
col1, col2 = st.columns([5, 1])

with col2:
    # é¢„æµ‹æŒ‰é’®
    predict_button = st.button("PUSH", type="primary")
    
    # ClearæŒ‰é’®
    def clear_values():
        st.session_state.clear_pressed = True
        st.session_state.prediction_result = None
        st.session_state.warnings = []
        st.session_state.individual_predictions = []
    
    clear_button = st.button("CLEAR", on_click=clear_values)

# åˆ›å»ºè¾“å…¥æ•°æ®DataFrame
input_data = pd.DataFrame([features])

# é¢„æµ‹æµç¨‹
if predict_button:
    log("="*40)
    log("å¼€å§‹æ–°é¢„æµ‹")
    
    try:
        # æ£€æŸ¥è¾“å…¥èŒƒå›´
        warnings = predictor.check_input_range(input_data)
        st.session_state.warnings = warnings
        
        # æ‰§è¡Œé¢„æµ‹ - ç°åœ¨ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨
        result, individual_preds = predictor.predict(input_data, return_individual=True)
        
        # ä¿å­˜ç»“æœ
        st.session_state.prediction_result = float(result[0])
        st.session_state.individual_predictions = individual_preds
        
        log(f"é¢„æµ‹æˆåŠŸå®Œæˆ: {st.session_state.prediction_result:.2f}")
        
    except Exception as e:
        log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")

# æ˜¾ç¤ºç»“æœ
with result_container:
    # ä¸»é¢„æµ‹ç»“æœ
    st.subheader("Char Yield (wt%)")
    
    if st.session_state.prediction_result is not None:
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        st.markdown(
            f"<div class='yield-result'>{st.session_state.prediction_result:.2f}</div>",
            unsafe_allow_html=True
        )
        
        # æ˜¾ç¤ºè­¦å‘Š
        if st.session_state.warnings:
            warning_html = "<div class='warning-box'><b>âš ï¸ è­¦å‘Š:</b> ä»¥ä¸‹è¾“å…¥å€¼è¶…å‡ºè®­ç»ƒèŒƒå›´ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§:<ul>"
            for warning in st.session_state.warnings:
                warning_html += f"<li>{warning}</li>"
            warning_html += "</ul></div>"
            st.markdown(warning_html, unsafe_allow_html=True)
        
        # æ ‡å‡†åŒ–å™¨çŠ¶æ€æç¤º
        if len(predictor.scalers) == len(predictor.models):
            st.markdown(
                "<div class='success-box'>âœ… æ¯ä¸ªå­æ¨¡å‹éƒ½ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯é åº¦é«˜ã€‚</div>",
                unsafe_allow_html=True
            )
        elif len(predictor.scalers) > 0:
            st.markdown(
                "<div class='warning-box'>âš ï¸ éƒ¨åˆ†å­æ¨¡å‹ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œéƒ¨åˆ†ä½¿ç”¨äº†æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨è½»å¾®åå·®ã€‚</div>",
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                "<div class='error-box'>âŒ æ²¡æœ‰æ‰¾åˆ°å­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨è¾ƒå¤§åå·®ã€‚</div>",
                unsafe_allow_html=True
            )
        
        # æ¨¡å‹è¯¦ç»†ä¿¡æ¯åŒºåŸŸ
        with st.expander("é¢„æµ‹è¯¦æƒ…", expanded=False):
            # æ˜¾ç¤ºå„ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
            if st.session_state.individual_predictions:
                col1, col2 = st.columns([3, 2])
                
                with col1:
                    st.write("### å„å­æ¨¡å‹é¢„æµ‹å€¼")
                    pred_df = pd.DataFrame({
                        'æ¨¡å‹': [f"æ¨¡å‹ {i+1}" for i in range(len(st.session_state.individual_predictions))],
                        'é¢„æµ‹å€¼': st.session_state.individual_predictions,
                        'åå·®': [p - st.session_state.prediction_result for p in st.session_state.individual_predictions]
                    })
                    st.dataframe(pred_df.style.format({
                        'é¢„æµ‹å€¼': '{:.2f}',
                        'åå·®': '{:.2f}'
                    }))
                    
                    # è®¡ç®—æ ‡å‡†å·®
                    std_dev = np.std(st.session_state.individual_predictions)
                    st.write(f"æ¨¡å‹é—´é¢„æµ‹æ ‡å‡†å·®: {std_dev:.2f}")
                    if std_dev > 3.0:
                        st.warning("âš ï¸ æ ‡å‡†å·®è¾ƒå¤§ï¼Œè¡¨ç¤ºæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§è¾ƒä½")
                    elif std_dev < 1.0:
                        st.success("âœ… æ ‡å‡†å·®è¾ƒå°ï¼Œè¡¨ç¤ºæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§é«˜")
                
                with col2:
                    # ç®€å•æŸ±çŠ¶å›¾
                    st.write("### é¢„æµ‹åˆ†å¸ƒ")
                    fig, ax = plt.subplots(figsize=(4, 3))
                    ax.bar(pred_df['æ¨¡å‹'], pred_df['é¢„æµ‹å€¼'], color='skyblue')
                    ax.axhline(st.session_state.prediction_result, color='red', linestyle='--', label='å¹³å‡å€¼')
                    ax.set_ylabel('é¢„æµ‹å€¼')
                    ax.set_xticklabels(pred_df['æ¨¡å‹'], rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)
            
            # æ˜¾ç¤ºè¾“å…¥ç‰¹å¾åŠå…¶é‡è¦æ€§
            if predictor.feature_importance is not None:
                st.write("### è¾“å…¥ç‰¹å¾å€¼åŠå…¶é‡è¦æ€§æ’å")
                
                # åˆå¹¶ç‰¹å¾é‡è¦æ€§å’Œè¾“å…¥å€¼
                input_values = input_data.iloc[0].to_dict()
                importance_data = predictor.feature_importance.copy()
                
                # è®¡ç®—æ’å
                importance_data['æ’å'] = importance_data.index + 1
                
                # æ·»åŠ è¾“å…¥å€¼åˆ—
                importance_data['è¾“å…¥å€¼'] = importance_data['Feature'].map(input_values)
                # è°ƒæ•´æ˜¾ç¤ºåˆ—
                display_df = importance_data[['æ’å', 'Feature', 'è¾“å…¥å€¼', 'Importance']]
                display_df.columns = ['æ’å', 'ç‰¹å¾', 'è¾“å…¥å€¼', 'é‡è¦æ€§å¾—åˆ†']
                
                st.dataframe(display_df.style.format({
                    'è¾“å…¥å€¼': '{:.1f}',
                    'é‡è¦æ€§å¾—åˆ†': '{:.4f}'
                }))
                
                # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§å›¾
                importance_img = predictor.get_feature_importance_plot()
                if importance_img:
                    st.image(importance_img, caption="ç‰¹å¾é‡è¦æ€§åˆ†æ", use_column_width=True)
                
                # æç¤ºæœ€é‡è¦ç‰¹å¾çš„å½±å“
                top_feature = importance_data['Feature'].iloc[0]
                top_value = input_values.get(top_feature)
                
                if top_feature == 'PT(Â°C)':
                    if top_value < 350:
                        st.info(f"ğŸ” æ‚¨çš„çƒ­è§£æ¸©åº¦({top_value}Â°C)è¾ƒä½ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è¾ƒé«˜çš„ç„¦ç‚­äº§ç‡ã€‚")
                    elif top_value > 600:
                        st.info(f"ğŸ” æ‚¨çš„çƒ­è§£æ¸©åº¦({top_value}Â°C)è¾ƒé«˜ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è¾ƒä½çš„ç„¦ç‚­äº§ç‡ã€‚")
                
                elif top_feature == 'RT(min)':
                    if top_value > 45:
                        st.info(f"ğŸ” æ‚¨çš„åœç•™æ—¶é—´({top_value}åˆ†é’Ÿ)è¾ƒé•¿ï¼Œè¿™é€šå¸¸ä¼šå¢åŠ ç„¦ç‚­äº§ç‡ã€‚")
                    elif top_value < 10:
                        st.info(f"ğŸ” æ‚¨çš„åœç•™æ—¶é—´({top_value}åˆ†é’Ÿ)è¾ƒçŸ­ï¼Œè¿™é€šå¸¸ä¼šå‡å°‘ç„¦ç‚­äº§ç‡ã€‚")

# æ·»åŠ æ¨¡å‹ä¿¡æ¯åŒºåŸŸ
with st.expander("About the Model", expanded=False):
    # å·¦å³ä¸¤åˆ—å¸ƒå±€
    info_col, chart_col = st.columns([1, 1])
    
    with info_col:
        st.write("### æ¨¡å‹ä¿¡æ¯")
        model_info = predictor.get_model_info()
        for key, value in model_info.items():
            st.write(f"**{key}:** {value}")
        
        st.write("### å…³é”®å½±å“å› ç´ ")
        st.markdown("""
        * **çƒ­è§£æ¸©åº¦(PT)**: æ›´é«˜çš„æ¸©åº¦é€šå¸¸ä¼šé™ä½ç„¦ç‚­äº§ç‡
        * **åœç•™æ—¶é—´(RT)**: æ›´é•¿çš„åœç•™æ—¶é—´é€šå¸¸ä¼šå¢åŠ ç„¦ç‚­äº§ç‡
        * **ç”Ÿç‰©è´¨æˆåˆ†**: ç¢³å«é‡å’Œç°åˆ†å«é‡æ˜¾è‘—å½±å“æœ€ç»ˆäº§ç‡
        """)
        
        # æ·»åŠ æ ‡å‡†åŒ–å™¨ä¿¡æ¯
        st.write("### æ ‡å‡†åŒ–å™¨ä½¿ç”¨æƒ…å†µ")
        if len(predictor.scalers) == len(predictor.models):
            st.success(f"âœ… æ‰€æœ‰ {len(predictor.models)} ä¸ªå­æ¨¡å‹éƒ½ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨")
        elif len(predictor.scalers) > 0:
            st.warning(f"âš ï¸ æ‰¾åˆ° {len(predictor.scalers)}/{len(predictor.models)} ä¸ªå­æ¨¡å‹æ ‡å‡†åŒ–å™¨")
        else:
            st.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å­æ¨¡å‹æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨ä»£æ›¿")
        
    with chart_col:
        if predictor.feature_importance is not None:
            importance_img = predictor.get_feature_importance_plot()
            if importance_img:
                st.image(importance_img, caption="Feature Importance Analysis", use_column_width=True)
        
        # æ·»åŠ æ ‡å‡†å·®ä¿¡æ¯
        if 'individual_predictions' in st.session_state and st.session_state.individual_predictions:
            std_dev = np.std(st.session_state.individual_predictions)
            
            # åˆ›å»ºç®€å•çš„æ ‡å‡†å·®æŒ‡ç¤ºå™¨
            fig, ax = plt.subplots(figsize=(8, 2))
            std_level = min(std_dev / 5.0, 1.0)  # æ ‡å‡†åŒ–ä¸º0-1èŒƒå›´
            
            # ç»˜åˆ¶ä¸€ä¸ªç®€å•çš„ä»ªè¡¨
            ax.barh(0, std_level, color='red' if std_level > 0.6 else 'orange' if std_level > 0.3 else 'green')
            ax.barh(0, 1, color='lightgrey', alpha=0.3)
            ax.set_xlim(0, 1)
            ax.set_ylim(-0.5, 0.5)
            ax.set_yticks([])
            ax.set_xticks([0, 0.25, 0.5, 0.75, 1])
            ax.set_xticklabels(['æä½³', 'è‰¯å¥½', 'ä¸€èˆ¬', 'è¾ƒå·®', 'å¾ˆå·®'])
            ax.set_title(f'æ¨¡å‹ä¸€è‡´æ€§: {std_dev:.2f}', fontsize=14)
            
            plt.tight_layout()
            st.pyplot(fig)
            
            # æ·»åŠ è§£é‡Š
            if std_dev < 1.5:
                st.success("âœ… å­æ¨¡å‹é¢„æµ‹ç»“æœä¸€è‡´æ€§é«˜ï¼Œå¢åŠ äº†æœ€ç»ˆé¢„æµ‹çš„å¯é æ€§")
            elif std_dev < 3.0:
                st.info("â„¹ï¸ å­æ¨¡å‹é¢„æµ‹ç»“æœä¸€è‡´æ€§ä¸­ç­‰ï¼Œæœ€ç»ˆé¢„æµ‹å¯ä¿¡åº¦é€‚ä¸­")
            else:
                st.warning("âš ï¸ å­æ¨¡å‹é¢„æµ‹ç»“æœå·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®è€ƒè™‘æ¨¡å‹ä¸ç¡®å®šæ€§")

# æ·»åŠ è°ƒè¯•ä¿¡æ¯åŒºåŸŸï¼ˆéšè—ä½†å¯å±•å¼€ï¼‰
with st.expander("Debug Information", expanded=False):
    st.write("### è¾“å…¥ç‰¹å¾")
    st.write(input_data)
    
    st.write("### æ¨¡å‹ä¿¡æ¯")
    if hasattr(predictor, 'models') and predictor.models:
        st.write(f"æ¨¡å‹æ•°é‡: {len(predictor.models)}")
    if hasattr(predictor, 'scalers') and predictor.scalers:
        st.write(f"æ ‡å‡†åŒ–å™¨æ•°é‡: {len(predictor.scalers)}")
    if hasattr(predictor, 'model_weights') and predictor.model_weights is not None:
        st.write(f"æƒé‡å½¢çŠ¶: {predictor.model_weights.shape}")
    if hasattr(predictor, 'feature_names') and predictor.feature_names:
        st.write(f"ç‰¹å¾åç§°: {predictor.feature_names}")
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–å™¨ä¿¡æ¯
    if hasattr(predictor, 'final_scaler') and predictor.final_scaler is not None:
        if hasattr(predictor.final_scaler, 'mean_'):
            st.write("### æœ€ç»ˆæ ‡å‡†åŒ–å™¨å‡å€¼")
            mean_df = pd.DataFrame({
                'ç‰¹å¾': predictor.feature_names,
                'å‡å€¼': predictor.final_scaler.mean_
            })
            st.dataframe(mean_df)
        
        if hasattr(predictor.final_scaler, 'scale_'):
            st.write("### æœ€ç»ˆæ ‡å‡†åŒ–å™¨æ ‡å‡†å·®")
            scale_df = pd.DataFrame({
                'ç‰¹å¾': predictor.feature_names,
                'æ ‡å‡†å·®': predictor.final_scaler.scale_
            })
            st.dataframe(scale_df)
    
    # æ˜¾ç¤ºæ ‡å‡†åŒ–å‰åçš„æ•°æ®
    if hasattr(predictor, 'final_scaler') and predictor.final_scaler is not None:
        st.write("### æ ‡å‡†åŒ–å‰åå¯¹æ¯”")
        original_data = input_data.iloc[0].to_dict()
        scaled_data = predictor.final_scaler.transform(input_data)[0]
        
        compare_df = pd.DataFrame({
            'ç‰¹å¾': predictor.feature_names,
            'åŸå§‹å€¼': [original_data.get(f) for f in predictor.feature_names],
            'æ ‡å‡†åŒ–å': scaled_data
        })
        
        st.dataframe(compare_df.style.format({
            'åŸå§‹å€¼': '{:.2f}',
            'æ ‡å‡†åŒ–å': '{:.4f}'
        }))

# æ·»åŠ æŠ€æœ¯è¯´æ˜åŒºåŸŸ
with st.expander("Technical Notes", expanded=False):
    st.write("### å…³äºé¢„æµ‹ç²¾åº¦")
    st.markdown("""
    è¯¥æ¨¡å‹ä½¿ç”¨CatBoostå›å½’å™¨é›†æˆè¿›è¡Œé¢„æµ‹ï¼Œæ¨¡å‹çš„æµ‹è¯•é›†RMSEä¸º3.39ï¼Œè¿™æ„å‘³ç€ï¼š
    
    - åœ¨æµ‹è¯•é›†ä¸Šï¼Œå¹³å‡é¢„æµ‹è¯¯å·®çº¦ä¸ºÂ±3.39ä¸ªç™¾åˆ†ç‚¹
    - å®é™…é¢„æµ‹å¯èƒ½ä¼šç•¥é«˜æˆ–ç•¥ä½äºè¿™ä¸ªèŒƒå›´
    - ç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®èŒƒå›´è¾¹ç¼˜çš„è¾“å…¥å€¼ï¼ˆå¦‚ä½æ¸©æˆ–æé«˜æ¸©ï¼‰å¯èƒ½ä¼šæœ‰æ›´å¤§è¯¯å·®
    
    ### é¢„æµ‹æ”¹è¿›
    
    æ­¤ç‰ˆæœ¬ä¿®å¤äº†å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜ï¼Œæ¯ä¸ªæ¨¡å‹ç°åœ¨ä½¿ç”¨å…¶å¯¹åº”çš„æ ‡å‡†åŒ–å™¨å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¿™æ˜¾è‘—æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚
    
    ### ä½¿ç”¨å»ºè®®
    
    - å¯¹äºå…³é”®å†³ç­–ï¼Œå»ºè®®å¤šè°ƒæ•´è¾“å…¥å‚æ•°ï¼Œè§‚å¯Ÿé¢„æµ‹å€¼çš„å˜åŒ–è¶‹åŠ¿
    - é¢„æµ‹ç»“æœæœ€å¥½ç»“åˆå®é™…ç»éªŒå’Œå®éªŒå®¤éªŒè¯
    - å½“è¾“å…¥å‚æ•°è¶…å‡ºè®­ç»ƒèŒƒå›´æ—¶ï¼Œè¯·è°¨æ…ä½¿ç”¨é¢„æµ‹ç»“æœ
    """)
    
    # æ·»åŠ æ¸©åº¦æ•æ„Ÿæ€§åˆ†æ
    if 'prediction_result' in st.session_state and st.session_state.prediction_result is not None:
        current_temp = features.get('PT(Â°C)', 500.0)
        current_result = st.session_state.prediction_result
        
        st.write("### æ¸©åº¦æ•æ„Ÿæ€§åˆ†æ")
        st.info(f"è¿™å°†æ˜¾ç¤ºåœ¨å½“å‰æ¸©åº¦({current_temp}Â°C)é™„è¿‘ï¼Œæ¸©åº¦å˜åŒ–å¯¹ç„¦ç‚­äº§ç‡çš„å½±å“ã€‚")
        
        temp_range = [-50, -25, 0, 25, 50]
        temp_results = []
        
        run_analysis = st.button("è¿è¡Œæ¸©åº¦æ•æ„Ÿæ€§åˆ†æ")
        
        if run_analysis:
            for delta in temp_range:
                # åˆ›å»ºä¸€ä¸ªæ–°çš„è¾“å…¥å¤åˆ¶
                temp_input = input_data.copy()
                new_temp = current_temp + delta
                temp_input['PT(Â°C)'] = new_temp
                
                # é¢„æµ‹
                try:
                    result = predictor.predict(temp_input)[0]
                    temp_results.append((new_temp, result))
                except Exception as e:
                    log(f"æ¸©åº¦ {new_temp}Â°C åˆ†æå¤±è´¥: {str(e)}")
            
            if temp_results:
                # åˆ›å»ºåˆ†æè¡¨æ ¼
                temp_df = pd.DataFrame(temp_results, columns=['æ¸©åº¦(Â°C)', 'é¢„æµ‹ç„¦ç‚­äº§ç‡(%)'])
                st.dataframe(temp_df.style.format({
                    'æ¸©åº¦(Â°C)': '{:.1f}',
                    'é¢„æµ‹ç„¦ç‚­äº§ç‡(%)': '{:.2f}'
                }))
                
                # ç»˜åˆ¶æ¸©åº¦æ•æ„Ÿæ€§å›¾
                fig, ax = plt.subplots(figsize=(8, 4))
                ax.plot([t[0] for t in temp_results], [t[1] for t in temp_results], 
                        'o-', color='blue', linewidth=2)
                
                # æ ‡è®°å½“å‰æ¸©åº¦ç‚¹
                current_idx = next((i for i, (t, _) in enumerate(temp_results) if t == current_temp), None)
                if current_idx is not None:
                    ax.plot(current_temp, temp_results[current_idx][1], 'o', color='red', markersize=10)
                
                ax.set_xlabel('æ¸©åº¦ (Â°C)')
                ax.set_ylabel('é¢„æµ‹ç„¦ç‚­äº§ç‡ (%)')
                ax.set_title('æ¸©åº¦å¯¹ç„¦ç‚­äº§ç‡çš„å½±å“')
                ax.grid(True, linestyle='--', alpha=0.7)
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # æ·»åŠ æ¸©åº¦æ•æ„Ÿæ€§è§£é‡Š
                if len(temp_results) > 2:
                    first_temp, first_yield = temp_results[0]
                    last_temp, last_yield = temp_results[-1]
                    temp_diff = last_temp - first_temp
                    yield_diff = last_yield - first_yield
                    
                    # è®¡ç®—æ¸©åº¦å˜åŒ–ç‡
                    temp_sensitivity = yield_diff / temp_diff if temp_diff != 0 else 0
                    
                    st.write(f"**æ¸©åº¦æ•æ„Ÿæ€§:** {temp_sensitivity:.4f} %/Â°C")
                    if temp_sensitivity < -0.1:
                        st.write("æ¸©åº¦å¯¹ç„¦ç‚­äº§ç‡æœ‰æ˜¾è‘—è´Ÿé¢å½±å“ï¼Œæé«˜æ¸©åº¦ä¼šé™ä½äº§ç‡ã€‚")
                    elif temp_sensitivity > 0.1:
                        st.write("æ¸©åº¦å¯¹ç„¦ç‚­äº§ç‡æœ‰æ˜¾è‘—æ­£é¢å½±å“ï¼Œæé«˜æ¸©åº¦ä¼šå¢åŠ äº§ç‡ã€‚")
                    else:
                        st.write("æ¸©åº¦å¯¹ç„¦ç‚­äº§ç‡å½±å“è¾ƒå°ã€‚")

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: gray; font-size: 14px;">
Â© 2023 Biomass Pyrolysis Research Team. All rights reserved.<br>
ä½¿ç”¨CatBoosté›†æˆæ¨¡å‹é¢„æµ‹ç”Ÿç‰©è´¨çƒ­è§£äº§ç‡ | æ¨¡å‹ç²¾åº¦: RÂ² = 0.93, RMSE = 3.39
</div>
""", unsafe_allow_html=True)
