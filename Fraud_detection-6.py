# -*- coding: utf-8 -*-
"""
Biomass Pyrolysis Yield Forecast using CatBoost Ensemble Models
ä¿®å¤ç‰ˆæœ¬ - è§£å†³å°æ•°ç²¾åº¦é—®é¢˜å’Œå­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import glob
import joblib
import json
import traceback
import matplotlib.pyplot as plt
from datetime import datetime
import io
from PIL import Image

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title='Biomass Pyrolysis Yield Prediction',
    page_icon='ğŸ”¥',
    layout='wide',
    initial_sidebar_state='expanded'
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown(
    """
    <style>
    /* å…¨å±€å­—ä½“è®¾ç½® */
    html, body, [class*="css"] {
        font-size: 16px !important;
    }
    
    /* æ ‡é¢˜ */
    .main-title {
        text-align: center;
        font-size: 32px !important;
        font-weight: bold;
        margin-bottom: 20px;
        color: white !important;
    }
    
    /* åŒºåŸŸæ ·å¼ */
    .section-header {
        color: white;
        font-weight: bold;
        font-size: 22px;
        text-align: center;
        padding: 10px;
        border-radius: 8px;
        margin-bottom: 15px;
    }
    
    /* è¾“å…¥æ ‡ç­¾æ ·å¼ */
    .input-label {
        padding: 5px;
        border-radius: 5px;
        margin-bottom: 5px;
        font-size: 18px;
        color: white;
    }
    
    /* ç»“æœæ˜¾ç¤ºæ ·å¼ */
    .yield-result {
        background-color: #1E1E1E;
        color: white;
        font-size: 36px;
        font-weight: bold;
        text-align: center;
        padding: 15px;
        border-radius: 8px;
        margin-top: 20px;
    }
    
    /* å¼ºåˆ¶åº”ç”¨ç™½è‰²èƒŒæ™¯åˆ°è¾“å…¥æ¡† */
    [data-testid="stNumberInput"] input {
        background-color: white !important;
        color: black !important;
    }
    
    /* å¢å¤§æŒ‰é’®çš„å­—ä½“ */
    .stButton button {
        font-size: 18px !important;
    }
    
    /* è­¦å‘Šæ ·å¼ */
    .warning-box {
        background-color: rgba(255, 165, 0, 0.2);
        border-left: 5px solid orange;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* é”™è¯¯æ ·å¼ */
    .error-box {
        background-color: rgba(255, 0, 0, 0.2);
        border-left: 5px solid red;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* æˆåŠŸæ ·å¼ */
    .success-box {
        background-color: rgba(0, 128, 0, 0.2);
        border-left: 5px solid green;
        padding: 10px;
        margin: 10px 0;
        border-radius: 5px;
    }
    
    /* æ—¥å¿—æ ·å¼ */
    .log-container {
        height: 300px;
        overflow-y: auto;
        background-color: #1E1E1E;
        color: #00FF00;
        font-family: 'Courier New', monospace;
        padding: 10px;
        border-radius: 5px;
        font-size: 14px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# åˆ›å»ºä¾§è¾¹æ æ—¥å¿—åŒºåŸŸ
log_container = st.sidebar.container()
log_container.markdown("<h3>æ‰§è¡Œæ—¥å¿—</h3>", unsafe_allow_html=True)
log_text = st.sidebar.empty()

# åˆå§‹åŒ–æ—¥å¿—å­—ç¬¦ä¸²
if 'log_messages' not in st.session_state:
    st.session_state.log_messages = []

def log(message):
    """è®°å½•æ—¥å¿—åˆ°ä¾§è¾¹æ å’Œä¼šè¯çŠ¶æ€"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    st.session_state.log_messages.append(log_entry)
    # åªä¿ç•™æœ€è¿‘çš„100æ¡æ—¥å¿—
    if len(st.session_state.log_messages) > 100:
        st.session_state.log_messages = st.session_state.log_messages[-100:]
    
    # æ›´æ–°æ—¥å¿—æ˜¾ç¤º
    log_text.markdown(
        f"<div class='log-container'>{'<br>'.join(st.session_state.log_messages)}</div>", 
        unsafe_allow_html=True
    )

# ä¸»æ ‡é¢˜
st.markdown("<h1 class='main-title'>Prediction of crop biomass pyrolysis yield based on CatBoost ensemble modeling</h1>", unsafe_allow_html=True)

class CorrectedEnsemblePredictor:
    """ä¿®å¤ç‰ˆé›†æˆæ¨¡å‹é¢„æµ‹å™¨ - è§£å†³å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜"""
    
    def __init__(self):
        self.models = []
        self.scalers = []  # æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨
        self.final_scaler = None  # æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼ˆå¤‡ç”¨ï¼‰
        self.model_weights = None
        self.feature_names = None
        self.target_name = "Char Yield(%)"
        self.metadata = None
        self.model_dir = None
        self.feature_importance = None
        self.training_ranges = {}
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    def find_model_directory(self):
        """æŸ¥æ‰¾æ¨¡å‹ç›®å½•çš„å¤šç§æ–¹æ³•"""
        # æ¨¡å‹ç›®å½•å¯èƒ½çš„è·¯å¾„
        possible_dirs = [
            # ç›´æ¥è·¯å¾„
            "Char_Yield_Model",
            "Char_Yield%_Model",
            # ç›¸å¯¹è·¯å¾„
            "./Char_Yield_Model",
            "./Char_Yield%_Model",
            "../Char_Yield_Model",
            "../Char_Yield%_Model",
            # ç»å¯¹è·¯å¾„ç¤ºä¾‹
            "C:/Users/HWY/Desktop/æ–¹-3/Char_Yield_Model",
            "C:/Users/HWY/Desktop/æ–¹-3/Char_Yield%_Model"
        ]
        
        # å°è¯•æ‰€æœ‰å¯èƒ½è·¯å¾„
        for dir_path in possible_dirs:
            if os.path.exists(dir_path) and os.path.isdir(dir_path):
                log(f"æ‰¾åˆ°æ¨¡å‹ç›®å½•: {dir_path}")
                return os.path.abspath(dir_path)
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡æ¨¡å‹æ–‡ä»¶æ¨æ–­
        try:
            model_files = glob.glob("**/model_*.joblib", recursive=True)
            if model_files:
                model_dir = os.path.dirname(os.path.dirname(model_files[0]))
                log(f"åŸºäºæ¨¡å‹æ–‡ä»¶æ¨æ–­æ¨¡å‹ç›®å½•: {model_dir}")
                return model_dir
        except Exception as e:
            log(f"é€šè¿‡æ¨¡å‹æ–‡ä»¶æ¨æ–­ç›®å½•æ—¶å‡ºé”™: {str(e)}")
        
        # å½“å‰ç›®å½•ä½œä¸ºæœ€åçš„é€€è·¯
        log("è­¦å‘Š: æ— æ³•æ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œå°†ä½¿ç”¨å½“å‰ç›®å½•")
        return os.getcwd()
    
    def load_feature_importance(self):
        """åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®"""
        try:
            # å°è¯•ä»CSVæ–‡ä»¶åŠ è½½ç‰¹å¾é‡è¦æ€§
            importance_csv = os.path.join(self.model_dir, "feature_importance.csv")
            if os.path.exists(importance_csv):
                importance_df = pd.read_csv(importance_csv)
                self.feature_importance = importance_df
                log(f"å·²åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®ï¼Œå…± {len(importance_df)} ä¸ªç‰¹å¾")
                return True
            
            # å¦‚æœCSVä¸å­˜åœ¨ï¼Œå°è¯•ä»å…ƒæ•°æ®ä¸­åŠ è½½
            if self.metadata and 'feature_importance' in self.metadata:
                importance_data = self.metadata['feature_importance']
                self.feature_importance = pd.DataFrame(importance_data)
                log(f"ä»å…ƒæ•°æ®åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®")
                return True
            
            # å°è¯•é€šè¿‡åŠ è½½çš„æ¨¡å‹è®¡ç®—ç‰¹å¾é‡è¦æ€§
            if self.models and self.model_weights is not None and self.feature_names:
                log("é€šè¿‡æ¨¡å‹è®¡ç®—ç‰¹å¾é‡è¦æ€§")
                importance = np.zeros(len(self.feature_names))
                for i, model in enumerate(self.models):
                    try:
                        model_importance = model.get_feature_importance()
                        importance += model_importance * self.model_weights[i]
                    except Exception as e:
                        log(f"è·å–æ¨¡å‹ {i} ç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {str(e)}")
                
                self.feature_importance = pd.DataFrame({
                    'Feature': self.feature_names,
                    'Importance': importance
                }).sort_values('Importance', ascending=False)
                
                log(f"è®¡ç®—å¾—åˆ°ç‰¹å¾é‡è¦æ€§æ•°æ®ï¼Œæœ€é‡è¦ç‰¹å¾: {self.feature_importance['Feature'].iloc[0]}")
                return True
                
            log("è­¦å‘Š: æ— æ³•åŠ è½½æˆ–è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            return False
        except Exception as e:
            log(f"åŠ è½½ç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {str(e)}")
            return False
    
    def extract_training_ranges(self):
        """ä»æ ‡å‡†åŒ–å™¨ä¸­æå–è®­ç»ƒæ•°æ®èŒƒå›´"""
        if not hasattr(self.final_scaler, 'mean_') or not hasattr(self.final_scaler, 'scale_'):
            log("è­¦å‘Š: æ ‡å‡†åŒ–å™¨æ²¡æœ‰å‡å€¼æˆ–æ ‡å‡†å·®ä¿¡æ¯")
            return
        
        if not self.feature_names:
            log("è­¦å‘Š: æ— æ³•è·å–ç‰¹å¾åç§°")
            return
        
        # æå–ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
        means = self.final_scaler.mean_
        stds = self.final_scaler.scale_
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„95%ç½®ä¿¡åŒºé—´ (å‡å€¼Â±2æ ‡å‡†å·®)
        for i, feature in enumerate(self.feature_names):
            if i < len(means) and i < len(stds):
                mean_val = means[i]
                std_val = stds[i]
                self.training_ranges[feature] = {
                    'mean': mean_val,
                    'std': std_val,
                    'min': mean_val - 2 * std_val,  # è¿‘ä¼¼95%ç½®ä¿¡åŒºé—´ä¸‹é™
                    'max': mean_val + 2 * std_val,  # è¿‘ä¼¼95%ç½®ä¿¡åŒºé—´ä¸Šé™
                }
        
        if self.training_ranges:
            log(f"å·²æå– {len(self.training_ranges)} ä¸ªç‰¹å¾çš„è®­ç»ƒèŒƒå›´")
    
    def load_model(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶ï¼ŒåŒ…æ‹¬æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨"""
        try:
            # 1. æŸ¥æ‰¾æ¨¡å‹ç›®å½•
            self.model_dir = self.find_model_directory()
            log(f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {self.model_dir}")
            
            # 2. åŠ è½½å…ƒæ•°æ®
            metadata_path = os.path.join(self.model_dir, 'metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
                
                # è·å–ç‰¹å¾åç§°å’Œç›®æ ‡å˜é‡
                self.feature_names = self.metadata.get('feature_names', None)
                if self.metadata.get('target_name'):
                    self.target_name = self.metadata['target_name']
                
                log(f"ä»å…ƒæ•°æ®åŠ è½½ç‰¹å¾åˆ—è¡¨: {self.feature_names}")
                log(f"ç›®æ ‡å˜é‡: {self.target_name}")
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ {metadata_path}")
                # ä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨ - å¿…é¡»ä¸æ¨¡å‹è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
                self.feature_names = [
                    'C(%)', 'H(%)', 'O(%)', 'N(%)', 'Ash(%)', 'VM(%)', 'FC(%)', 
                    'PT(Â°C)', 'HR(â„ƒ/min)', 'RT(min)'
                ]
                log(f"ä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨: {self.feature_names}")
            
            # 3. åŠ è½½æ¨¡å‹
            models_dir = os.path.join(self.model_dir, 'models')
            if os.path.exists(models_dir):
                model_files = sorted(glob.glob(os.path.join(models_dir, 'model_*.joblib')))
                if model_files:
                    for model_file in model_files:
                        model = joblib.load(model_file)
                        self.models.append(model)
                        log(f"åŠ è½½æ¨¡å‹: {os.path.basename(model_file)}")
                else:
                    log(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶åœ¨ {models_dir}")
                    return False
            else:
                log(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
                return False
            
            # 4. åŠ è½½æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨ - è¿™æ˜¯å…³é”®ä¿®å¤ç‚¹
            scalers_dir = os.path.join(self.model_dir, 'scalers')
            if os.path.exists(scalers_dir):
                scaler_files = sorted(glob.glob(os.path.join(scalers_dir, 'scaler_*.joblib')))
                if scaler_files:
                    for scaler_file in scaler_files:
                        scaler = joblib.load(scaler_file)
                        self.scalers.append(scaler)
                        log(f"åŠ è½½å­æ¨¡å‹æ ‡å‡†åŒ–å™¨: {os.path.basename(scaler_file)}")
                else:
                    log(f"è­¦å‘Š: æœªæ‰¾åˆ°å­æ¨¡å‹æ ‡å‡†åŒ–å™¨æ–‡ä»¶åœ¨ {scalers_dir}")
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨ç›®å½•: {scalers_dir}")
            
            # 5. åŠ è½½æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
            final_scaler_path = os.path.join(self.model_dir, 'final_scaler.joblib')
            if os.path.exists(final_scaler_path):
                self.final_scaler = joblib.load(final_scaler_path)
                log(f"åŠ è½½æœ€ç»ˆæ ‡å‡†åŒ–å™¨: {final_scaler_path}")
                
                # æ‰“å°æ ‡å‡†åŒ–å™¨ä¿¡æ¯
                if hasattr(self.final_scaler, 'mean_'):
                    log(f"ç‰¹å¾å‡å€¼: {self.final_scaler.mean_}")
                if hasattr(self.final_scaler, 'scale_'):
                    log(f"ç‰¹å¾æ ‡å‡†å·®: {self.final_scaler.scale_}")
                
                # æå–è®­ç»ƒæ•°æ®èŒƒå›´
                self.extract_training_ranges()
            else:
                log(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ€ç»ˆæ ‡å‡†åŒ–å™¨æ–‡ä»¶ {final_scaler_path}")
            
            # 6. åŠ è½½æƒé‡
            weights_path = os.path.join(self.model_dir, 'model_weights.npy')
            if os.path.exists(weights_path):
                self.model_weights = np.load(weights_path)
                log(f"åŠ è½½æƒé‡æ–‡ä»¶: {weights_path}")
            else:
                self.model_weights = np.ones(len(self.models)) / len(self.models)
                log("è­¦å‘Š: æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œä½¿ç”¨å‡ç­‰æƒé‡")
            
            # 7. åŠ è½½ç‰¹å¾é‡è¦æ€§
            self.load_feature_importance()
            
            # éªŒè¯åŠ è½½çŠ¶æ€
            log(f"æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹å’Œ {len(self.scalers)} ä¸ªå­æ¨¡å‹æ ‡å‡†åŒ–å™¨")
            
            # ç‰¹åˆ«æ ‡è®°æ ‡å‡†åŒ–å™¨é—®é¢˜
            if len(self.models) != len(self.scalers):
                log(f"è­¦å‘Š: æ¨¡å‹æ•°é‡ ({len(self.models)}) ä¸æ ‡å‡†åŒ–å™¨æ•°é‡ ({len(self.scalers)}) ä¸åŒ¹é…")
                
            return True
            
        except Exception as e:
            log(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return False
    
    def check_input_range(self, input_df):
        """æ£€æŸ¥è¾“å…¥å€¼æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®èŒƒå›´å†…"""
        warnings = []
        
        if not self.training_ranges:
            log("è­¦å‘Š: æ²¡æœ‰è®­ç»ƒæ•°æ®èŒƒå›´ä¿¡æ¯ï¼Œè·³è¿‡èŒƒå›´æ£€æŸ¥")
            return warnings
        
        for feature, range_info in self.training_ranges.items():
            if feature in input_df.columns:
                value = input_df[feature].iloc[0]
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºè®­ç»ƒæ•°æ®çš„95%ç½®ä¿¡åŒºé—´
                if value < range_info['min'] or value > range_info['max']:
                    warning = f"{feature}: {value:.2f} (è¶…å‡ºè®­ç»ƒèŒƒå›´ {range_info['min']:.2f} - {range_info['max']:.2f})"
                    warnings.append(warning)
                    log(f"è­¦å‘Š: {warning}")
        
        return warnings
    
    def predict(self, input_features, return_individual=False):
        """ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨è¿›è¡Œé¢„æµ‹"""
        try:
            # éªŒè¯æ¨¡å‹ç»„ä»¶
            if not self.models or len(self.models) == 0:
                log("é”™è¯¯: æ²¡æœ‰åŠ è½½æ¨¡å‹")
                return np.array([0.0])
            
            # ç¡®ä¿è¾“å…¥ç‰¹å¾åŒ…å«æ‰€æœ‰å¿…è¦ç‰¹å¾
            missing_features = []
            if self.feature_names:
                for feature in self.feature_names:
                    if feature not in input_features.columns:
                        missing_features.append(feature)
            
            if missing_features:
                missing_str = ", ".join(missing_features)
                log(f"é”™è¯¯: è¾“å…¥ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾: {missing_str}")
                return np.array([0.0])
            
            # æŒ‰ç…§æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºé‡æ–°æ’åˆ—
            if self.feature_names:
                input_ordered = input_features[self.feature_names].copy()
                log("è¾“å…¥ç‰¹å¾å·²æŒ‰ç…§è®­ç»ƒæ—¶çš„é¡ºåºæ’åˆ—")
            else:
                input_ordered = input_features
                log("è­¦å‘Š: æ²¡æœ‰ç‰¹å¾åç§°åˆ—è¡¨ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥é¡ºåº")
            
            # è®°å½•è¾“å…¥æ•°æ®
            log(f"é¢„æµ‹è¾“å…¥æ•°æ®: {input_ordered.iloc[0].to_dict()}")
            
            # ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å’Œå¯¹åº”çš„æ ‡å‡†åŒ–å™¨è¿›è¡Œé¢„æµ‹
            individual_predictions = []
            all_predictions = np.zeros((input_ordered.shape[0], len(self.models)))
            
            # æ£€æŸ¥æ ‡å‡†åŒ–å™¨æ˜¯å¦è¶³å¤Ÿ
            scalers_available = len(self.scalers) > 0
            
            for i, model in enumerate(self.models):
                try:
                    # ä½¿ç”¨å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if scalers_available and i < len(self.scalers):
                        X_scaled = self.scalers[i].transform(input_ordered)
                        log(f"æ¨¡å‹ {i} ä½¿ç”¨å¯¹åº”çš„æ ‡å‡†åŒ–å™¨")
                    else:
                        # å¦‚æœæ²¡æœ‰å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨
                        if self.final_scaler:
                            X_scaled = self.final_scaler.transform(input_ordered)
                            log(f"æ¨¡å‹ {i} ä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨")
                        else:
                            log(f"é”™è¯¯: æ¨¡å‹ {i} æ²¡æœ‰å¯ç”¨çš„æ ‡å‡†åŒ–å™¨")
                            continue
                    
                    pred = model.predict(X_scaled)
                    all_predictions[:, i] = pred
                    individual_predictions.append(float(pred[0]))
                    log(f"æ¨¡å‹ {i} é¢„æµ‹ç»“æœ: {pred[0]:.2f}")
                except Exception as e:
                    log(f"æ¨¡å‹ {i} é¢„æµ‹æ—¶å‡ºé”™: {str(e)}")
                    # å¦‚æœæŸä¸ªæ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨å…¶ä»–æ¨¡å‹çš„å¹³å‡å€¼
                    if i > 0:
                        avg_pred = np.mean(all_predictions[:, :i], axis=1)
                        all_predictions[:, i] = avg_pred
                        individual_predictions.append(float(avg_pred[0]))
                        log(f"æ¨¡å‹ {i} ä½¿ç”¨ä¹‹å‰æ¨¡å‹çš„å¹³å‡å€¼: {avg_pred[0]:.2f}")
            
            # è®¡ç®—åŠ æƒå¹³å‡
            weighted_pred = np.sum(all_predictions * self.model_weights.reshape(1, -1), axis=1)
            log(f"æœ€ç»ˆåŠ æƒé¢„æµ‹ç»“æœ: {weighted_pred[0]:.2f}")
            
            if return_individual:
                return weighted_pred, individual_predictions
            else:
                return weighted_pred
            
        except Exception as e:
            log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return np.array([0.0])
    
    def get_feature_importance_plot(self):
        """ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾"""
        if self.feature_importance is None or len(self.feature_importance) == 0:
            return None
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(8, 5))
            
            # æå–æ•°æ®
            importance_df = self.feature_importance.sort_values('Importance', ascending=True)
            features = importance_df['Feature'].tolist()
            importance = importance_df['Importance'].tolist()
            
            # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
            ax.barh(features, importance, color='skyblue')
            
            # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title('Feature Importance', fontsize=14)
            ax.set_xlabel('Importance Score', fontsize=12)
            ax.set_ylabel('Feature', fontsize=12)
            
            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()
            
            # å°†å›¾è¡¨è½¬æ¢ä¸ºå›¾åƒ
            buf = io.BytesIO()
            fig.savefig(buf, format='png', dpi=100)
            buf.seek(0)
            
            # ä½¿ç”¨PILæ‰“å¼€å›¾åƒå¹¶è¿”å›
            img = Image.open(buf)
            return img
            
        except Exception as e:
            log(f"åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯æ‘˜è¦"""
        info = {
            "æ¨¡å‹ç±»å‹": "CatBoosté›†æˆæ¨¡å‹",
            "æ¨¡å‹æ•°é‡": len(self.models),
            "ç‰¹å¾æ•°é‡": len(self.feature_names) if self.feature_names else 0,
            "ç›®æ ‡å˜é‡": self.target_name
        }
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        if self.metadata and 'performance' in self.metadata:
            performance = self.metadata['performance']
            info["æµ‹è¯•é›†RÂ²"] = f"{performance.get('test_r2', 'N/A'):.4f}"
            info["æµ‹è¯•é›†RMSE"] = f"{performance.get('test_rmse', 'N/A'):.2f}"
        
        # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
        if self.feature_importance is not None and len(self.feature_importance) > 0:
            top_features = self.feature_importance.head(3)
            info["é‡è¦ç‰¹å¾"] = ", ".join(top_features['Feature'].tolist())
        
        # æ·»åŠ æ ‡å‡†åŒ–å™¨ä¿¡æ¯
        info["å­æ¨¡å‹æ ‡å‡†åŒ–å™¨æ•°é‡"] = len(self.scalers)
        
        return info

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = CorrectedEnsemblePredictor()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'clear_pressed' not in st.session_state:
    st.session_state.clear_pressed = False
if 'prediction_result' not in st.session_state:
    st.session_state.prediction_result = None
if 'warnings' not in st.session_state:
    st.session_state.warnings = []
if 'individual_predictions' not in st.session_state:
    st.session_state.individual_predictions = []

# å®šä¹‰é»˜è®¤å€¼ - ä»ç”¨æˆ·æˆªå›¾ä¸­æå–
default_values = {
    "C(%)": 46.00,  # ä½¿ç”¨ä¸¤ä½å°æ•°ç²¾åº¦
    "H(%)": 5.50,
    "O(%)": 55.20,
    "N(%)": 0.60,
    "Ash(%)": 6.60,
    "VM(%)": 81.10,
    "FC(%)": 10.30,
    "PT(Â°C)": 500.00,  # ä½¿ç”¨å®é™…æµ‹è¯•å€¼
    "HR(â„ƒ/min)": 10.00,
    "RT(min)": 60.00
}

# ç‰¹å¾åˆ†ç±»
feature_categories = {
    "Ultimate Analysis": ["C(%)", "H(%)", "O(%)", "N(%)"],
    "Proximate Analysis": ["Ash(%)", "VM(%)", "FC(%)"],
    "Pyrolysis Conditions": ["PT(Â°C)", "HR(â„ƒ/min)", "RT(min)"]
}

# é¢œè‰²é…ç½®
category_colors = {
    "Ultimate Analysis": "#DAA520",  # é»„è‰²
    "Proximate Analysis": "#32CD32",  # ç»¿è‰²
    "Pyrolysis Conditions": "#FF7F50"  # æ©™è‰²
}

# åˆ›å»ºä¸‰åˆ—å¸ƒå±€
col1, col2, col3 = st.columns(3)

# ä½¿ç”¨å­—å…¸å­˜å‚¨æ‰€æœ‰è¾“å…¥å€¼
features = {}

# Ultimate Analysis - ç¬¬ä¸€åˆ—
with col1:
    category = "Ultimate Analysis"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            # å…³é”®ä¿®æ”¹: è®¾ç½®æ­¥é•¿ä¸º0.01ä»¥æ”¯æŒä¸¤ä½å°æ•°
            features[feature] = st.number_input(
                "", 
                min_value=0.00, 
                max_value=100.00, 
                value=float(value), 
                step=0.01,  # è®¾ç½®ä¸º0.01å…è®¸ä¸¤ä½å°æ•°è¾“å…¥
                key=f"{category}_{feature}", 
                format="%.2f",  # å¼ºåˆ¶æ˜¾ç¤ºä¸¤ä½å°æ•°
                label_visibility="collapsed"
            )

# Proximate Analysis - ç¬¬äºŒåˆ—
with col2:
    category = "Proximate Analysis"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            # å…³é”®ä¿®æ”¹: è®¾ç½®æ­¥é•¿ä¸º0.01ä»¥æ”¯æŒä¸¤ä½å°æ•°
            features[feature] = st.number_input(
                "", 
                min_value=0.00, 
                max_value=100.00, 
                value=float(value), 
                step=0.01,  # è®¾ç½®ä¸º0.01å…è®¸ä¸¤ä½å°æ•°è¾“å…¥
                key=f"{category}_{feature}", 
                format="%.2f",  # å¼ºåˆ¶æ˜¾ç¤ºä¸¤ä½å°æ•°
                label_visibility="collapsed"
            )

# Pyrolysis Conditions - ç¬¬ä¸‰åˆ—
with col3:
    category = "Pyrolysis Conditions"
    color = category_colors[category]
    st.markdown(f"<div class='section-header' style='background-color: {color};'>{category}</div>", unsafe_allow_html=True)
    
    for feature in feature_categories[category]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"{category}_{feature}", default_values[feature])
        
        # æ ¹æ®ç‰¹å¾è®¾ç½®èŒƒå›´
        if feature == "PT(Â°C)":
            min_val, max_val = 200.00, 900.00
        elif feature == "HR(â„ƒ/min)":
            min_val, max_val = 1.00, 100.00
        elif feature == "RT(min)":
            min_val, max_val = 0.00, 120.00
        else:
            min_val, max_val = 0.00, 100.00
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: {color};'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            # å…³é”®ä¿®æ”¹: è®¾ç½®æ­¥é•¿ä¸º0.01ä»¥æ”¯æŒä¸¤ä½å°æ•°
            features[feature] = st.number_input(
                "", 
                min_value=float(min_val), 
                max_value=float(max_val), 
                value=float(value), 
                step=0.01,  # è®¾ç½®ä¸º0.01å…è®¸ä¸¤ä½å°æ•°è¾“å…¥
                key=f"{category}_{feature}", 
                format="%.2f",  # å¼ºåˆ¶æ˜¾ç¤ºä¸¤ä½å°æ•°
                label_visibility="collapsed"
            )

# é‡ç½®çŠ¶æ€
if st.session_state.clear_pressed:
    st.session_state.clear_pressed = False

# é¢„æµ‹ç»“æœæ˜¾ç¤ºåŒºåŸŸ
result_container = st.container()

# é¢„æµ‹æŒ‰é’®åŒºåŸŸ
col1, col2 = st.columns([5, 1])

with col2:
    # é¢„æµ‹æŒ‰é’®
    predict_button = st.button("PUSH", type="primary")
    
    # ClearæŒ‰é’®
    def clear_values():
        st.session_state.clear_pressed = True
        st.session_state.prediction_result = None
        st.session_state.warnings = []
        st.session_state.individual_predictions = []
    
    clear_button = st.button("CLEAR", on_click=clear_values)

# åˆ›å»ºè¾“å…¥æ•°æ®DataFrame
input_data = pd.DataFrame([features])

# é¢„æµ‹æµç¨‹
if predict_button:
    log("="*40)
    log("å¼€å§‹æ–°é¢„æµ‹")
    
    try:
        # æ£€æŸ¥è¾“å…¥èŒƒå›´
        warnings = predictor.check_input_range(input_data)
        st.session_state.warnings = warnings
        
        # æ‰§è¡Œé¢„æµ‹ - ç°åœ¨ä½¿ç”¨æ¯ä¸ªå­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨
        result, individual_preds = predictor.predict(input_data, return_individual=True)
        
        # ä¿å­˜ç»“æœ
        st.session_state.prediction_result = float(result[0])
        st.session_state.individual_predictions = individual_preds
        
        log(f"é¢„æµ‹æˆåŠŸå®Œæˆ: {st.session_state.prediction_result:.2f}")
        
    except Exception as e:
        log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")

# æ˜¾ç¤ºç»“æœ
with result_container:
    # ä¸»é¢„æµ‹ç»“æœ
    st.subheader("Char Yield (wt%)")
    
    if st.session_state.prediction_result is not None:
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        st.markdown(
            f"<div class='yield-result'>{st.session_state.prediction_result:.2f}</div>",
            unsafe_allow_html=True
        )
        
        # æ˜¾ç¤ºè­¦å‘Š
        if st.session_state.warnings:
            warning_html = "<div class='warning-box'><b>âš ï¸ è­¦å‘Š:</b> ä»¥ä¸‹è¾“å…¥å€¼è¶…å‡ºè®­ç»ƒèŒƒå›´ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§:<ul>"
            for warning in st.session_state.warnings:
                warning_html += f"<li>{warning}</li>"
            warning_html += "</ul></div>"
            st.markdown(warning_html, unsafe_allow_html=True)
        
        # æ ‡å‡†åŒ–å™¨çŠ¶æ€æç¤º
        if len(predictor.scalers) == len(predictor.models):
            st.markdown(
                "<div class='success-box'>âœ… æ¯ä¸ªå­æ¨¡å‹éƒ½ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯é åº¦é«˜ã€‚</div>",
                unsafe_allow_html=True
            )
        elif len(predictor.scalers) > 0:
            st.markdown(
                "<div class='warning-box'>âš ï¸ éƒ¨åˆ†å­æ¨¡å‹ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œéƒ¨åˆ†ä½¿ç”¨äº†æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨è½»å¾®åå·®ã€‚</div>",
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                "<div class='error-box'>âŒ æ²¡æœ‰æ‰¾åˆ°å­æ¨¡å‹å¯¹åº”çš„æ ‡å‡†åŒ–å™¨ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨ï¼Œé¢„æµ‹ç»“æœå¯èƒ½å­˜åœ¨è¾ƒå¤§åå·®ã€‚</div>",
                unsafe_allow_html=True
            )
        
        # æ¨¡å‹è¯¦ç»†ä¿¡æ¯åŒºåŸŸ
        with st.expander("é¢„æµ‹è¯¦æƒ…", expanded=False):
            # æ˜¾ç¤ºå„ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
            if st.session_state.individual_predictions:
                col1, col2 = st.columns([3, 2])
                
                with col1:
                    st.write("### å„å­æ¨¡å‹é¢„æµ‹å€¼")
                    pred_df = pd.DataFrame({
                        'æ¨¡å‹': [f"æ¨¡å‹ {i+1}" for i in range(len(st.session_state.individual_predictions))],
                        'é¢„æµ‹å€¼': st.session_state.individual_predictions,
                        'åå·®': [p - st.session_state.prediction_result for p in st.session_state.individual_predictions]
                    })
                    st.dataframe(pred_df.style.format({
                        'é¢„æµ‹å€¼': '{:.2f}',
                        'åå·®': '{:.2f}'
                    }))
                    
                    # è®¡ç®—æ ‡å‡†å·®
                    std_dev = np.std(st.session_state.individual_predictions)
                    st.write(f"æ¨¡å‹é—´é¢„æµ‹æ ‡å‡†å·®: {std_dev:.2f}")
                    if std_dev > 3.0:
                        st.warning("âš ï¸ æ ‡å‡†å·®è¾ƒå¤§ï¼Œè¡¨ç¤ºæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§è¾ƒä½")
                    elif std_dev < 1.0:
                        st.success("âœ… æ ‡å‡†å·®è¾ƒå°ï¼Œè¡¨ç¤ºæ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§é«˜")
                with col2:
                    # æ˜¾ç¤ºå­æ¨¡å‹é¢„æµ‹åˆ†å¸ƒå›¾
                    st.write("### é¢„æµ‹åˆ†å¸ƒ")
                    fig, ax = plt.subplots(figsize=(5, 3))
                    ax.hist(st.session_state.individual_predictions, bins=5, alpha=0.7, color='skyblue')
                    ax.axvline(st.session_state.prediction_result, color='red', linestyle='--', linewidth=2, label='æœ€ç»ˆé¢„æµ‹')
                    ax.set_xlabel('é¢„æµ‹å€¼')
                    ax.set_ylabel('é¢‘ç‡')
                    ax.legend()
                    st.pyplot(fig)
            
            # æ˜¾ç¤ºè¾“å…¥ç‰¹å¾è¡¨
            st.write("### è¾“å…¥ç‰¹å¾")
            input_df = pd.DataFrame([features])
            
            # æ ¼å¼åŒ–ä¸ºä¸¤ä½å°æ•°æ˜¾ç¤º
            display_df = input_df.applymap(lambda x: f"{x:.2f}")
            st.dataframe(display_df)

# ç‰¹å¾é‡è¦æ€§å’Œæ¨¡å‹ä¿¡æ¯éƒ¨åˆ†
col1, col2 = st.columns([1, 1])

with col1:
    # ç‰¹å¾é‡è¦æ€§éƒ¨åˆ†
    st.subheader("ç‰¹å¾é‡è¦æ€§")
    
    if predictor.feature_importance is not None:
        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§è¡¨æ ¼
        importance_df = predictor.feature_importance.copy()
        
        # æ ¼å¼åŒ–é‡è¦æ€§åˆ†æ•°ï¼Œä½¿ç”¨4ä½å°æ•°
        formatted_df = importance_df.copy()
        formatted_df['Importance'] = formatted_df['Importance'].apply(lambda x: f"{x:.4f}")
        
        st.dataframe(formatted_df, use_container_width=True)
        
        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§å›¾
        importance_img = predictor.get_feature_importance_plot()
        if importance_img:
            st.image(importance_img, use_column_width=True)
        
        # æä¾›ç‰¹å¾é‡è¦æ€§çš„æ´å¯Ÿ
        st.markdown("#### é‡è¦ç‰¹å¾æ´å¯Ÿ")
        
        # è·å–å‰ä¸¤ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_features = importance_df['Feature'].tolist()[:2]
        
        if 'PT(Â°C)' in top_features:
            st.info("""
            ğŸ“Œ **æ¸©åº¦(PT)** æ˜¯å½±å“äº§ç‡çš„æœ€é‡è¦å› ç´ ï¼Œè¿™ä¸çƒ­è§£ç†è®ºä¸€è‡´ï¼š
            - è¾ƒä½æ¸©åº¦ä¸‹ï¼Œç”Ÿç‰©è´¨é™è§£ä¸å®Œå…¨ï¼Œå¯¼è‡´ç„¦ç‚­äº§ç‡è¾ƒé«˜
            - éšç€æ¸©åº¦å‡é«˜ï¼Œçƒ­è§£ååº”æ›´å½»åº•ï¼Œæ°”ä½“äº§ç‰©å¢åŠ ï¼Œç„¦ç‚­äº§ç‡ä¸‹é™
            """)
        
        if 'RT(min)' in top_features:
            st.info("""
            ğŸ“Œ **åœç•™æ—¶é—´(RT)** æ˜¾è‘—å½±å“çƒ­è§£ç¨‹åº¦ï¼š
            - è¾ƒçŸ­çš„åœç•™æ—¶é—´å¯èƒ½å¯¼è‡´çƒ­è§£ä¸å®Œå…¨
            - è¾ƒé•¿çš„åœç•™æ—¶é—´å…è®¸æ›´å¤šçš„æŒ¥å‘åˆ†é‡Šæ”¾ï¼Œå‡å°‘ç„¦ç‚­äº§ç‡
            """)
    else:
        st.warning("æ— æ³•åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®")

with col2:
    # å…³äºæ¨¡å‹éƒ¨åˆ†
    st.subheader("å…³äºæ¨¡å‹")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = predictor.get_model_info()
    
    # åˆ›å»ºä¿¡æ¯è¡¨
    for key, value in model_info.items():
        st.markdown(f"**{key}**: {value}")
    
    # æ ‡å‡†åŒ–å™¨çŠ¶æ€
    st.markdown("#### æ ‡å‡†åŒ–å™¨çŠ¶æ€")
    if len(predictor.scalers) == len(predictor.models):
        st.success(f"âœ… æ‰€æœ‰ {len(predictor.models)} ä¸ªå­æ¨¡å‹éƒ½ä½¿ç”¨äº†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨")
    elif len(predictor.scalers) > 0:
        st.warning(f"âš ï¸ æ‰¾åˆ° {len(predictor.scalers)}/{len(predictor.models)} ä¸ªå­æ¨¡å‹æ ‡å‡†åŒ–å™¨")
    else:
        st.error("âŒ æœªæ‰¾åˆ°å­æ¨¡å‹æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨æœ€ç»ˆæ ‡å‡†åŒ–å™¨")
    
    # å­æ¨¡å‹ä¸æ ‡å‡†å·®å¯è§†åŒ–
    st.markdown("#### é¢„æµ‹æ ‡å‡†å·®")
    if st.session_state.individual_predictions:
        std_dev = np.std(st.session_state.individual_predictions)
        
        # åˆ›å»ºè¿›åº¦æ¡è¡¨ç¤ºæ ‡å‡†å·®
        st.progress(min(std_dev / 5.0, 1.0))  # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        
        # æ ¹æ®æ ‡å‡†å·®å¤§å°æ˜¾ç¤ºä¸åŒæ¶ˆæ¯
        if std_dev < 1.0:
            st.success(f"é¢„æµ‹ä¸€è‡´æ€§é«˜ (æ ‡å‡†å·® = {std_dev:.2f})")
        elif std_dev < 3.0:
            st.info(f"é¢„æµ‹ä¸€è‡´æ€§ä¸­ç­‰ (æ ‡å‡†å·® = {std_dev:.2f})")
        else:
            st.warning(f"é¢„æµ‹ä¸€è‡´æ€§ä½ (æ ‡å‡†å·® = {std_dev:.2f})")

# è°ƒè¯•ä¿¡æ¯åŒºåŸŸ
with st.expander("è°ƒè¯•ä¿¡æ¯", expanded=False):
    st.markdown("### è¾“å…¥ç‰¹å¾è¯¦æƒ…")
    # æ˜¾ç¤ºå¸¦ä¸¤ä½å°æ•°æ ¼å¼çš„è¾“å…¥ç‰¹å¾
    formatted_features = {k: f"{v:.2f}" for k, v in features.items()}
    st.json(formatted_features)
    
    st.markdown("### æ¨¡å‹ä¿¡æ¯")
    st.json({
        "æ¨¡å‹æ•°é‡": len(predictor.models),
        "æ ‡å‡†åŒ–å™¨æ•°é‡": len(predictor.scalers),
        "ç‰¹å¾æ•°é‡": len(predictor.feature_names) if predictor.feature_names else 0,
        "ç‰¹å¾åˆ—è¡¨": predictor.feature_names,
        "æ¨¡å‹ç›®å½•": predictor.model_dir
    })
    
    st.markdown("### æ ‡å‡†åŒ–å™¨ä¿¡æ¯")
    if predictor.final_scaler and hasattr(predictor.final_scaler, 'mean_'):
        scaler_info = {
            "å‡å€¼": predictor.final_scaler.mean_.tolist(),
            "æ ‡å‡†å·®": predictor.final_scaler.scale_.tolist() if hasattr(predictor.final_scaler, 'scale_') else None
        }
        st.json(scaler_info)
    else:
        st.warning("æœ€ç»ˆæ ‡å‡†åŒ–å™¨ä¿¡æ¯ä¸å¯ç”¨")

# æŠ€æœ¯è¯´æ˜åŒºåŸŸ
with st.expander("æŠ€æœ¯è¯´æ˜", expanded=False):
    st.markdown("""
    ### é¢„æµ‹ç²¾åº¦è¯´æ˜
    
    æœ¬æ¨¡å‹æ˜¯åŸºäºCatBoostçš„é›†æˆå­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡10ä¸ªå­æ¨¡å‹å…±åŒé¢„æµ‹ä»¥æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†çº¦0.93çš„RÂ²å’Œ3.39çš„RMSEã€‚
    
    #### å·²ä¿®å¤çš„é—®é¢˜
    
    1. **å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜**: åº”ç”¨æ­£ç¡®åŠ è½½å¹¶åº”ç”¨æ¯ä¸ªå­æ¨¡å‹çš„æ ‡å‡†åŒ–å™¨ï¼Œç¡®ä¿ç‰¹å¾çš„æ ‡å‡†åŒ–ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚
    2. **è¾“å…¥ç²¾åº¦é—®é¢˜**: å…è®¸è¾“å…¥ä¸¤ä½å°æ•°è€Œä¸æ˜¯ä¸€ä½ï¼Œå‡å°‘èˆå…¥è¯¯å·®ã€‚
    
    #### ä½¿ç”¨å»ºè®®
    
    1. å°½é‡ä½¿ç”¨åœ¨è®­ç»ƒèŒƒå›´å†…çš„è¾“å…¥å€¼ï¼Œè¶…å‡ºèŒƒå›´çš„é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®ã€‚
    2. å¯¹äºç”Ÿç‰©è´¨çƒ­è§£ï¼Œæ¸©åº¦(PT)å’Œåœç•™æ—¶é—´(RT)æ˜¯æœ€å…³é”®çš„å‚æ•°ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨è¿™äº›å‚æ•°çš„è®¾ç½®ã€‚
    3. å¦‚æœå¤šä¸ªå­æ¨¡å‹çš„é¢„æµ‹å·®å¼‚è¾ƒå¤§(æ ‡å‡†å·®>3)ï¼Œè¡¨æ˜å½“å‰è¾“å…¥æ¡ä»¶ä¸‹çš„é¢„æµ‹å¯èƒ½ä¸ç¨³å®šã€‚
    """)

# æ¸©åº¦æ•æ„Ÿæ€§åˆ†æ
with st.expander("æ¸©åº¦æ•æ„Ÿæ€§åˆ†æ", expanded=False):
    st.markdown("### åˆ†ææ¸©åº¦å¯¹äº§ç‡çš„å½±å“")
    
    # æ¸©åº¦èŒƒå›´æ»‘å—
    temp_range = st.slider("æ¸©åº¦èŒƒå›´(Â°C)", 
                          min_value=200, 
                          max_value=900, 
                          value=(300, 700),
                          step=50)
    
    # æ¸©åº¦æ­¥é•¿
    temp_step = st.selectbox("æ¸©åº¦æ­¥é•¿", options=[10, 25, 50, 100], index=1)
    
    # æ‰§è¡Œåˆ†ææŒ‰é’®
    if st.button("è¿è¡Œæ¸©åº¦æ•æ„Ÿæ€§åˆ†æ"):
        # åˆ›å»ºæ¸©åº¦åºåˆ—
        temps = np.arange(temp_range[0], temp_range[1] + 1, temp_step)
        
        # åˆ›å»ºä¿å­˜å½“å‰è¾“å…¥ç‰¹å¾çš„å‰¯æœ¬
        base_features = features.copy()
        
        # ç»“æœå®¹å™¨
        results = []
        
        # æ‰§è¡Œé¢„æµ‹
        for temp in temps:
            temp_features = base_features.copy()
            temp_features['PT(Â°C)'] = temp
            
            # åˆ›å»ºè¾“å…¥DataFrame
            temp_input = pd.DataFrame([temp_features])
            
            # é¢„æµ‹
            try:
                pred = predictor.predict(temp_input)
                results.append((temp, float(pred[0])))
            except Exception as e:
                st.error(f"æ¸©åº¦ {temp}Â°C é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºç»“æœ
        if results:
            # åˆ›å»ºDataFrame
            result_df = pd.DataFrame(results, columns=['æ¸©åº¦(Â°C)', 'é¢„æµ‹äº§ç‡(%)'])
            
            # æ˜¾ç¤ºè¡¨æ ¼
            st.dataframe(result_df.style.format({
                'æ¸©åº¦(Â°C)': '{:.0f}',
                'é¢„æµ‹äº§ç‡(%)': '{:.2f}'
            }))
            
            # ç»˜åˆ¶æ›²çº¿
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.plot(result_df['æ¸©åº¦(Â°C)'], result_df['é¢„æµ‹äº§ç‡(%)'], marker='o', linewidth=2)
            ax.set_xlabel('æ¸©åº¦(Â°C)', fontsize=12)
            ax.set_ylabel('é¢„æµ‹äº§ç‡(%)', fontsize=12)
            ax.set_title('æ¸©åº¦å¯¹äº§ç‡çš„å½±å“', fontsize=14)
            ax.grid(True, linestyle='--', alpha=0.7)
            
            # æ·»åŠ å½“å‰æ¸©åº¦æ ‡è®°
            current_temp = base_features['PT(Â°C)']
            if temp_range[0] <= current_temp <= temp_range[1]:
                # æ‰¾åˆ°æœ€æ¥è¿‘çš„é¢„æµ‹ç‚¹
                closest_idx = np.abs(result_df['æ¸©åº¦(Â°C)'] - current_temp).argmin()
                closest_temp = result_df.iloc[closest_idx]['æ¸©åº¦(Â°C)']
                closest_yield = result_df.iloc[closest_idx]['é¢„æµ‹äº§ç‡(%)']
                
                # æ ‡è®°å½“å‰æ¸©åº¦ç‚¹
                ax.scatter([closest_temp], [closest_yield], color='red', s=100, zorder=5, 
                           label=f'å½“å‰æ¸©åº¦: {current_temp:.0f}Â°C')
                ax.legend()
            
            st.pyplot(fig)
            
            # æ‰¾å‡ºæœ€å¤§å’Œæœ€å°äº§ç‡ç‚¹
            max_idx = result_df['é¢„æµ‹äº§ç‡(%)'].idxmax()
            min_idx = result_df['é¢„æµ‹äº§ç‡(%)'].idxmin()
            
            max_temp = result_df.iloc[max_idx]['æ¸©åº¦(Â°C)']
            max_yield = result_df.iloc[max_idx]['é¢„æµ‹äº§ç‡(%)']
            
            min_temp = result_df.iloc[min_idx]['æ¸©åº¦(Â°C)']
            min_yield = result_df.iloc[min_idx]['é¢„æµ‹äº§ç‡(%)']
            
            # æ˜¾ç¤ºåˆ†æç»“æœ
            st.markdown(f"""
            ### åˆ†æç»“æœ
            
            - åœ¨åˆ†æèŒƒå›´å†…ï¼Œäº§ç‡æœ€é«˜ç‚¹ä¸º: **{max_yield:.2f}%** (æ¸©åº¦ = {max_temp:.0f}Â°C)
            - åœ¨åˆ†æèŒƒå›´å†…ï¼Œäº§ç‡æœ€ä½ç‚¹ä¸º: **{min_yield:.2f}%** (æ¸©åº¦ = {min_temp:.0f}Â°C)
            - æ¸©åº¦å˜åŒ– 1Â°C å¹³å‡å¯¼è‡´äº§ç‡å˜åŒ–çº¦ {abs(max_yield - min_yield) / abs(max_temp - min_temp):.4f}%
            """)

# æ•°æ®éªŒè¯å»ºè®®
with st.expander("æ•°æ®éªŒè¯ä¸ç²¾åº¦å»ºè®®", expanded=False):
    st.markdown("""
    ### æé«˜é¢„æµ‹ç²¾åº¦çš„å»ºè®®
    
    1. **ç¡®ä¿æ•°æ®è´¨é‡**:
       - ä½¿ç”¨ä¸¤ä½å°æ•°è¾“å…¥å¯ä»¥å‡å°‘èˆå…¥è¯¯å·®
       - é€šè¿‡å®éªŒéªŒè¯è¾“å…¥çš„åˆ†ææ•°æ®
    
    2. **ä¼˜å…ˆå…³æ³¨é‡è¦ç‰¹å¾**:
       - çƒ­è§£æ¸©åº¦(PT)æ˜¯æœ€å…³é”®çš„å‚æ•°ï¼Œç¡®ä¿å…¶å‡†ç¡®æ€§
       - åœç•™æ—¶é—´(RT)æ˜¯ç¬¬äºŒé‡è¦çš„å‚æ•°ï¼Œéœ€è¦ç²¾ç¡®æ§åˆ¶
    
    3. **æ³¨æ„ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§**:
       - Cã€Hã€Oå«é‡é€šå¸¸ç›¸å…³ï¼Œç¡®ä¿å®ƒä»¬çš„æ€»å’Œåˆç†
       - VMå’ŒFCå«é‡ä¹Ÿåº”ä¸å…ƒç´ åˆ†æç»“æœç›¸ç¬¦
    
    4. **æ¨¡å‹çš„å±€é™æ€§**:
       - æ¨¡å‹ä¸»è¦åœ¨è®­ç»ƒæ•°æ®èŒƒå›´å†…æœ‰æ•ˆ
       - è¶…å‡ºèŒƒå›´çš„é¢„æµ‹ä¼šé€šè¿‡è­¦å‘Šæç¤ºï¼Œä½†å¯èƒ½ä¸å‡†ç¡®
    """)

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: gray; font-size: 14px;">
    Â© 2023 ç”Ÿç‰©è´¨çƒ­è§£äº§ç‡é¢„æµ‹ç³»ç»Ÿ | æ¨¡å‹ç²¾åº¦: RÂ² = 0.93, RMSE = 3.39 | é›†æˆ 10 ä¸ª CatBoost å­æ¨¡å‹<br>
    ç‰ˆæœ¬æ›´æ–°: æ”¯æŒä¸¤ä½å°æ•°è¾“å…¥ & ä¿®å¤å­æ¨¡å‹æ ‡å‡†åŒ–å™¨é—®é¢˜
</div>
""", unsafe_allow_html=True)





