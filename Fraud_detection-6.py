# -*- coding: utf-8 -*-
"""
Biomass Pyrolysis Yield Forecast using CatBoost Ensemble Models
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import glob
import joblib
import json
import traceback
import importlib.util

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title='Biomass Pyrolysis Yield Forecast',
    page_icon='ğŸ“Š',
    layout='wide'
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown(
    """
    <style>
    /* å…¨å±€å­—ä½“è®¾ç½® */
    html, body, [class*="css"] {
        font-size: 16px !important;
    }
    
    /* æ ‡é¢˜ */
    .main-title {
        text-align: center;
        font-size: 32px !important;
        font-weight: bold;
        margin-bottom: 20px;
        color: white !important;
    }
    
    /* åŒºåŸŸæ ·å¼ */
    .section-header {
        color: white;
        font-weight: bold;
        font-size: 22px;
        text-align: center;
        padding: 10px;
        border-radius: 8px;
        margin-bottom: 15px;
    }
    
    /* è¾“å…¥æ ‡ç­¾æ ·å¼ */
    .input-label {
        padding: 5px;
        border-radius: 5px;
        margin-bottom: 5px;
        font-size: 18px;
        color: white;
    }
    
    /* ç»“æœæ˜¾ç¤ºæ ·å¼ */
    .yield-result {
        background-color: #1E1E1E;
        color: white;
        font-size: 36px;
        font-weight: bold;
        text-align: center;
        padding: 15px;
        border-radius: 8px;
        margin-top: 20px;
    }
    
    /* å¼ºåˆ¶åº”ç”¨ç™½è‰²èƒŒæ™¯åˆ°è¾“å…¥æ¡† */
    [data-testid="stNumberInput"] input {
        background-color: white !important;
        color: black !important;
    }
    
    /* å¢å¤§æŒ‰é’®çš„å­—ä½“ */
    .stButton button {
        font-size: 18px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ä¸»æ ‡é¢˜
st.markdown("<h1 class='main-title'>Prediction of crop biomass pyrolysis yield based on CatBoost ensemble modeling</h1>", unsafe_allow_html=True)

# åˆ›å»ºä¾§è¾¹æ æ—¥å¿—åŒºåŸŸ
log_container = st.sidebar.container()
log_container.write("### è°ƒè¯•æ—¥å¿—")

def log(message):
    """è®°å½•è°ƒè¯•ä¿¡æ¯åˆ°ä¾§è¾¹æ """
    log_container.write(message)

# ç›´æ¥åŒ…å« DirectPredictor ç±»çš„å®šä¹‰ï¼Œè€Œä¸æ˜¯å°è¯•åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
class DirectPredictor:
    """ç›´æ¥åŠ è½½æ¨¡å‹æ–‡ä»¶è¿›è¡Œé¢„æµ‹çš„é¢„æµ‹å™¨"""
    
    def __init__(self):
        # æ ¹æ®è®­ç»ƒä»£ç è¾“å‡ºè®¾ç½®æ­£ç¡®çš„ç‰¹å¾é¡ºåº
        self.feature_names = ['C(%)', 'H(%)', 'O(%)', 'N(%)', 'Ash(%)', 'VM(%)', 'FC(%)', 'PT(Â°C)', 'HR(â„ƒ/min)', 'RT(min)']
        self.models = []
        self.model_weights = None
        self.scaler = None
        self.metadata = None
        self.feature_mapping = {}
        self.train_data_stats = {}
        self.model_dir = None
        
        # æŸ¥æ‰¾å¹¶åŠ è½½æ¨¡å‹
        self.load_model_components()
    
    def find_model_directories(self):
        """
        æŸ¥æ‰¾åŒ…å«æ¨¡å‹æ–‡ä»¶çš„ç›®å½•
        """
        model_dirs = []
        # æŸ¥æ‰¾åŒ…å«modelså­ç›®å½•å’Œmetadata.jsonçš„ç›®å½•
        for root, dirs, files in os.walk("."):
            if "models" in dirs and "metadata.json" in files:
                model_dirs.append(os.path.abspath(root))
        
        return model_dirs
    
    def load_model_components(self):
        """åŠ è½½æ¨¡å‹ç»„ä»¶"""
        try:
            # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
            model_dirs = self.find_model_directories()
            if not model_dirs:
                log("æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶")
                # å°è¯•ç›´æ¥æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                model_files = glob.glob("**/model_*.joblib", recursive=True)
                if model_files:
                    self.model_dir = os.path.dirname(os.path.dirname(model_files[0]))
                    log(f"åŸºäºæ¨¡å‹æ–‡ä»¶æ¨æ–­æ¨¡å‹ç›®å½•: {self.model_dir}")
                else:
                    log("æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
                    return False
            else:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹ç›®å½•
                self.model_dir = model_dirs[0]
                log(f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {self.model_dir}")
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = os.path.join(self.model_dir, 'metadata.json')
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
                metadata_features = self.metadata.get('feature_names', None)
                if metadata_features:
                    # éªŒè¯ç‰¹å¾é¡ºåº
                    if set(metadata_features) == set(self.feature_names):
                        # ä½¿ç”¨å…ƒæ•°æ®ä¸­çš„ç‰¹å¾é¡ºåº
                        self.feature_names = metadata_features
                    else:
                        log(f"è­¦å‘Šï¼šå…ƒæ•°æ®ä¸­çš„ç‰¹å¾ä¸é¢„æœŸä¸åŒ¹é…")
                log(f"åŠ è½½å…ƒæ•°æ®ï¼Œç‰¹å¾åç§°: {self.feature_names}")
                
                # ä»å…ƒæ•°æ®ä¸­æå–æ€§èƒ½ä¿¡æ¯
                if 'performance' in self.metadata:
                    self.performance = self.metadata['performance']
                    log(f"æ¨¡å‹æ€§èƒ½: RÂ²={self.performance.get('test_r2', 'unknown')}, RMSE={self.performance.get('test_rmse', 'unknown')}")
            else:
                log(f"æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
            
            # åŠ è½½æ¨¡å‹
            models_dir = os.path.join(self.model_dir, 'models')
            if os.path.exists(models_dir):
                model_files = sorted(glob.glob(os.path.join(models_dir, 'model_*.joblib')))
                if model_files:
                    for model_path in model_files:
                        try:
                            model = joblib.load(model_path)
                            self.models.append(model)
                            log(f"åŠ è½½æ¨¡å‹: {model_path}")
                        except Exception as e:
                            log(f"åŠ è½½æ¨¡å‹å¤±è´¥: {model_path}, é”™è¯¯: {e}")
                else:
                    log(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶åœ¨: {models_dir}")
                    # å°è¯•ç›´æ¥æŸ¥æ‰¾
                    model_files = sorted(glob.glob("**/model_*.joblib", recursive=True))
                    if model_files:
                        for model_path in model_files:
                            try:
                                model = joblib.load(model_path)
                                self.models.append(model)
                                log(f"åŠ è½½æ¨¡å‹: {model_path}")
                            except Exception as e:
                                log(f"åŠ è½½æ¨¡å‹å¤±è´¥: {model_path}, é”™è¯¯: {e}")
            else:
                log(f"æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•: {models_dir}")
                # å°è¯•ç›´æ¥æŸ¥æ‰¾
                model_files = sorted(glob.glob("**/model_*.joblib", recursive=True))
                if model_files:
                    for model_path in model_files:
                        try:
                            model = joblib.load(model_path)
                            self.models.append(model)
                            log(f"åŠ è½½æ¨¡å‹: {model_path}")
                        except Exception as e:
                            log(f"åŠ è½½æ¨¡å‹å¤±è´¥: {model_path}, é”™è¯¯: {e}")
            
            # åŠ è½½æƒé‡
            weights_path = os.path.join(self.model_dir, 'model_weights.npy')
            if os.path.exists(weights_path):
                self.model_weights = np.load(weights_path)
                log(f"åŠ è½½æƒé‡: {weights_path}")
            else:
                log(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {weights_path}")
                # å°è¯•ç›´æ¥æŸ¥æ‰¾
                weights_files = glob.glob("**/model_weights.npy", recursive=True)
                if weights_files:
                    self.model_weights = np.load(weights_files[0])
                    log(f"åŠ è½½æƒé‡: {weights_files[0]}")
                else:
                    if self.models:
                        self.model_weights = np.ones(len(self.models)) / len(self.models)
                        log("ä½¿ç”¨å‡ç­‰æƒé‡")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            scaler_path = os.path.join(self.model_dir, 'final_scaler.joblib')
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                log(f"åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
                
                # æå–æ ‡å‡†åŒ–å™¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”¨äºéªŒè¯
                if hasattr(self.scaler, 'mean_'):
                    self.train_data_stats['mean'] = self.scaler.mean_
                    log(f"ç‰¹å¾å‡å€¼: {self.scaler.mean_}")
                if hasattr(self.scaler, 'scale_'):
                    self.train_data_stats['scale'] = self.scaler.scale_
                    log(f"ç‰¹å¾æ ‡å‡†å·®: {self.scaler.scale_}")
            else:
                log(f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶: {scaler_path}")
                # å°è¯•ç›´æ¥æŸ¥æ‰¾
                scaler_files = glob.glob("**/final_scaler.joblib", recursive=True)
                if scaler_files:
                    self.scaler = joblib.load(scaler_files[0])
                    log(f"åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_files[0]}")
                    
                    # æå–æ ‡å‡†åŒ–å™¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”¨äºéªŒè¯
                    if hasattr(self.scaler, 'mean_'):
                        self.train_data_stats['mean'] = self.scaler.mean_
                    if hasattr(self.scaler, 'scale_'):
                        self.train_data_stats['scale'] = self.scaler.scale_
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
            if self.models:
                log(f"æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹")
                return True
            else:
                log("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹")
                return False
                
        except Exception as e:
            log(f"åŠ è½½æ¨¡å‹ç»„ä»¶æ—¶å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return False
    
    def check_input_range(self, X):
        """æ£€æŸ¥è¾“å…¥å€¼æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®èŒƒå›´å†…"""
        warnings = []
        
        if not hasattr(self.scaler, 'mean_') or not hasattr(self.scaler, 'scale_'):
            return warnings
            
        feature_mean = self.scaler.mean_
        feature_std = self.scaler.scale_
        
        # ä½¿ç”¨è½¬æ¢åçš„æ•°æ®è¿›è¡Œæ£€æŸ¥
        X_transformed = self.transform_input_to_model_order(X)
        log(f"æŒ‰æ¨¡å‹é¡ºåºçš„ç‰¹å¾æ•°æ®: {X_transformed.to_dict('records')}")
        
        # å‡è®¾ç‰¹å¾æ˜¯æ­£æ€åˆ†å¸ƒï¼Œè®¡ç®—å¤§è‡´çš„95%ç½®ä¿¡åŒºé—´
        for i, feature in enumerate(self.feature_names):
            if i < len(self.feature_names):
                input_val = X_transformed[feature].iloc[0]
                mean = feature_mean[i]
                std = feature_std[i]
                
                # æ£€æŸ¥æ˜¯å¦åç¦»å‡å€¼å¤ªå¤š
                lower_bound = mean - 2 * std
                upper_bound = mean + 2 * std
                
                if input_val < lower_bound or input_val > upper_bound:
                    log(f"è­¦å‘Š: {feature} = {input_val} è¶…å‡ºæ­£å¸¸èŒƒå›´ [{lower_bound:.2f}, {upper_bound:.2f}]")
                    warnings.append(f"{feature}: {input_val} (èŒƒå›´: {lower_bound:.2f}-{upper_bound:.2f})")
        
        return warnings
    
    def transform_input_to_model_order(self, X):
        """å°†è¾“å…¥ç‰¹å¾è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„é¡ºåº"""
        if not isinstance(X, pd.DataFrame):
            log("è¾“å…¥ä¸æ˜¯DataFrameæ ¼å¼")
            return X
            
        log(f"è¾“å…¥ç‰¹å¾é¡ºåº: {X.columns.tolist()}")
        log(f"æ¨¡å‹ç‰¹å¾é¡ºåº: {self.feature_names}")
        
        # åˆ›å»ºæ–°çš„DataFrameï¼Œä¿æŒæ¨¡å‹æœŸæœ›çš„ç‰¹å¾é¡ºåº
        X_new = pd.DataFrame(index=X.index)
        for feature in self.feature_names:
            # åœ¨UIç‰¹å¾ä¸­å¯»æ‰¾åŒ¹é…
            found = False
            # 1. ç›´æ¥åŒ¹é…
            if feature in X.columns:
                X_new[feature] = X[feature]
                found = True
            # 2. åŸºäºç‰¹å¾åå‰ç¼€åŒ¹é…
            else:
                feature_base = feature.split('(')[0]
                for col in X.columns:
                    col_base = col.split('(')[0]
                    if col_base == feature_base:
                        X_new[feature] = X[col]
                        log(f"åŸºäºå‰ç¼€åŒ¹é…ç‰¹å¾: {col} -> {feature}")
                        found = True
                        break
            
            if not found:
                log(f"è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°ç‰¹å¾ {feature} çš„å¯¹åº”è¾“å…¥")
                # ä½¿ç”¨é»˜è®¤å€¼
                X_new[feature] = 0.0
                
        return X_new
    
    def predict(self, X):
        """
        ä½¿ç”¨åŠ è½½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
            X: ç‰¹å¾æ•°æ®ï¼ŒDataFrameæ ¼å¼
        
        è¿”å›:
            é¢„æµ‹ç»“æœæ•°ç»„
        """
        try:
            if not self.models:
                log("æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼Œæ— æ³•é¢„æµ‹")
                return np.array([33.0])  # è¿”å›é»˜è®¤å€¼
            
            # å°†è¾“å…¥ç‰¹å¾è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„é¡ºåº
            X_model_order = self.transform_input_to_model_order(X)
            log(f"è½¬æ¢åçš„è¾“å…¥æ•°æ®:\n{X_model_order.to_dict('records')[0]}")
            
            # åº”ç”¨æ ‡å‡†åŒ–
            if self.scaler:
                log("åº”ç”¨æ ‡å‡†åŒ–")
                X_scaled = self.scaler.transform(X_model_order)
            else:
                log("æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                X_scaled = X_model_order.values
            
            # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            all_predictions = np.zeros((X_model_order.shape[0], len(self.models)))
            for i, model in enumerate(self.models):
                try:
                    pred = model.predict(X_scaled)
                    all_predictions[:, i] = pred
                    log(f"æ¨¡å‹ {i} é¢„æµ‹ç»“æœ: {pred[0]:.2f}")
                except Exception as e:
                    log(f"æ¨¡å‹ {i} é¢„æµ‹å¤±è´¥: {e}")
                    # ä½¿ç”¨å¹³å‡å€¼å¡«å……
                    if i > 0:
                        all_predictions[:, i] = np.mean(all_predictions[:, :i], axis=1)
            
            # è®¡ç®—åŠ æƒå¹³å‡é¢„æµ‹
            weighted_pred = np.sum(all_predictions * self.model_weights.reshape(1, -1), axis=1)
            log(f"æœ€ç»ˆåŠ æƒé¢„æµ‹ç»“æœ: {weighted_pred[0]:.2f}")
            
            return weighted_pred
            
        except Exception as e:
            log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            log(traceback.format_exc())
            return np.array([33.0])  # è¿”å›é»˜è®¤å€¼

# å°è¯•ä¿®å¤ç®€åŒ–é¢„æµ‹å™¨æ¨¡å—ä¸­çš„è¯­æ³•é”™è¯¯
def fix_simple_predictor():
    try:
        predictor_paths = glob.glob("**/simple_predictor.py", recursive=True)
        if not predictor_paths:
            log("æœªæ‰¾åˆ°simple_predictor.pyæ–‡ä»¶")
            return False
            
        predictor_path = predictor_paths[0]
        log(f"å°è¯•ä¿®å¤: {predictor_path}")
        
        # è¯»å–æ–‡ä»¶
        with open(predictor_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä¿®å¤ç±»åä¸­çš„ç™¾åˆ†å·å’Œseabornä¾èµ–
        fixed_content = content.replace("class Char_Yield%Predictor:", "class Char_YieldPredictor:")
        fixed_content = fixed_content.replace("import seaborn as sns", "# import seaborn as sns")
        fixed_content = fixed_content.replace("sns.kdeplot", "# sns.kdeplot")
        fixed_content = fixed_content.replace("sns.barplot", "# sns.barplot")
        
        # å†™å›æ–‡ä»¶
        with open(predictor_path, 'w', encoding='utf-8') as f:
            f.write(fixed_content)
            
        log("æˆåŠŸä¿®å¤simple_predictor.pyæ–‡ä»¶")
        return True
    except Exception as e:
        log(f"ä¿®å¤simple_predictor.pyæ—¶å‡ºé”™: {str(e)}")
        return False

# åŠ è½½ä¿®å¤simple_predictoræ¨¡å—
try:
    log("å°è¯•ä¿®å¤simple_predictor.pyæ–‡ä»¶ä¸­çš„è¯­æ³•é”™è¯¯")
    fix_simple_predictor()
except Exception as e:
    log(f"ä¿®å¤å°è¯•å¤±è´¥: {str(e)}")

# æŸ¥æ‰¾simple_predictor.pyæ–‡ä»¶
def find_predictor_module():
    """
    æŸ¥æ‰¾simple_predictor.pyæ¨¡å—
    """
    predictor_paths = glob.glob("**/simple_predictor.py", recursive=True)
    if predictor_paths:
        return predictor_paths[0]
    return None

# åŠ¨æ€åŠ è½½simple_predictoræ¨¡å—
def load_predictor_class():
    """
    åŠ¨æ€åŠ è½½simple_predictor.pyä¸­çš„é¢„æµ‹å™¨ç±»
    """
    try:
        # æŸ¥æ‰¾é¢„æµ‹å™¨æ¨¡å—
        predictor_path = find_predictor_module()
        if not predictor_path:
            log("æœªæ‰¾åˆ°simple_predictor.pyæ¨¡å—ï¼Œå°†ä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼")
            return None
        
        log(f"æ‰¾åˆ°é¢„æµ‹å™¨æ¨¡å—: {predictor_path}")
        
        # å°è¯•ç›´æ¥è¯»å–å¹¶æ£€æŸ¥é¢„æµ‹å™¨ç±»
        with open(predictor_path, 'r', encoding='utf-8') as f:
            content = f.readlines()
        
        # æŸ¥æ‰¾ç±»å®šä¹‰è¡Œ
        class_line = None
        for line in content:
            if "class" in line and "Predictor" in line:
                class_line = line.strip()
                log(f"æ‰¾åˆ°é¢„æµ‹å™¨ç±»å®šä¹‰: {class_line}")
                break
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯­æ³•é”™è¯¯
        if class_line and "%" in class_line:
            log("é¢„æµ‹å™¨ç±»ååŒ…å«éæ³•å­—ç¬¦ï¼Œæ— æ³•ç›´æ¥å¯¼å…¥")
            return None
        
        # å¯¼å…¥æ¨¡å—
        module_name = os.path.basename(predictor_path).replace(".py", "")
        spec = importlib.util.spec_from_file_location(module_name, predictor_path)
        module = importlib.util.module_from_spec(spec)
        
        try:
            spec.loader.exec_module(module)
        except SyntaxError as e:
            log(f"æ¨¡å—å­˜åœ¨è¯­æ³•é”™è¯¯: {e}")
            return None
        except ModuleNotFoundError as e:
            log(f"æ¨¡å—å¯¼å…¥é”™è¯¯: {e}")
            return None
        
        # æŸ¥æ‰¾é¢„æµ‹å™¨ç±»
        predictor_class = None
        for name in dir(module):
            if name.endswith("Predictor") and name != "Predictor":
                predictor_class = getattr(module, name)
                log(f"æ‰¾åˆ°é¢„æµ‹å™¨ç±»: {name}")
                break
        
        if predictor_class:
            # å®ä¾‹åŒ–é¢„æµ‹å™¨
            log("å®ä¾‹åŒ–é¢„æµ‹å™¨ç±»")
            predictor = predictor_class()
            log("é¢„æµ‹å™¨ç±»æˆåŠŸåŠ è½½")
            return predictor
        else:
            log("åœ¨æ¨¡å—ä¸­æœªæ‰¾åˆ°é¢„æµ‹å™¨ç±»")
            return None
            
    except Exception as e:
        log(f"åŠ è½½é¢„æµ‹å™¨ç±»æ—¶å‡ºé”™: {str(e)}")
        log(traceback.format_exc())
        return None

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = load_predictor_class()

# å¦‚æœæ— æ³•åŠ è½½simple_predictorï¼Œåˆ™ä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼
if predictor is None:
    log("ä½¿ç”¨ç›´æ¥æ¨¡å‹åŠ è½½æ–¹å¼")
    predictor = DirectPredictor()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'clear_pressed' not in st.session_state:
    st.session_state.clear_pressed = False

# å®šä¹‰é»˜è®¤å€¼å’ŒèŒƒå›´ - æŒ‰ç…§è®­ç»ƒçš„feature_namesé¡ºåºå®šä¹‰
default_values = {
    "C(%)": 45.0,
    "H(%)": 6.0,
    "O(%)": 40.0,
    "N(%)": 0.5,
    "Ash(%)": 5.0,
    "VM(%)": 75.0,
    "FC(%)": 15.0,
    "PT(Â°C)": 500.0,
    "HR(â„ƒ/min)": 20.0,
    "RT(min)": 20.0
}

# ç‰¹å¾åˆ†ç±» - æ ¹æ®æ¨¡å‹ç‰¹å¾åˆ†ç»„
feature_categories = {
    "Ultimate Analysis": ["C(%)", "H(%)", "O(%)", "N(%)"],
    "Proximate Analysis": ["Ash(%)", "VM(%)", "FC(%)"],
    "Pyrolysis Conditions": ["PT(Â°C)", "HR(â„ƒ/min)", "RT(min)"]
}

# ç‰¹å¾èŒƒå›´
feature_ranges = {
    "C(%)": (30.0, 80.0),
    "H(%)": (3.0, 10.0),
    "O(%)": (10.0, 60.0),
    "N(%)": (0.0, 5.0),
    "Ash(%)": (0.0, 25.0),
    "VM(%)": (40.0, 95.0),
    "FC(%)": (5.0, 40.0),
    "PT(Â°C)": (300.0, 900.0),
    "HR(â„ƒ/min)": (5.0, 100.0),
    "RT(min)": (5.0, 120.0)
}

# åˆ›å»ºä¸‰åˆ—å¸ƒå±€
col1, col2, col3 = st.columns(3)

# ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰è¾“å…¥å€¼
features = {}

# Ultimate Analysis (é»„è‰²åŒºåŸŸ) - ç¬¬ä¸€åˆ—
with col1:
    st.markdown("<div class='section-header' style='background-color: #DAA520;'>Ultimate Analysis</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Ultimate Analysis"]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"ultimate_{feature}", default_values[feature])
        
        min_val, max_val = feature_ranges[feature]
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #DAA520;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"ultimate_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Proximate Analysis (ç»¿è‰²åŒºåŸŸ) - ç¬¬äºŒåˆ—
with col2:
    st.markdown("<div class='section-header' style='background-color: #32CD32;'>Proximate Analysis</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Proximate Analysis"]:
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"proximate_{feature}", default_values[feature])
        
        min_val, max_val = feature_ranges[feature]
        
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #32CD32;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"proximate_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# Pyrolysis Conditions (æ©™è‰²åŒºåŸŸ) - ç¬¬ä¸‰åˆ—
with col3:
    st.markdown("<div class='section-header' style='background-color: #FF7F50;'>Pyrolysis Conditions</div>", unsafe_allow_html=True)
    
    for feature in feature_categories["Pyrolysis Conditions"]:
        # é‡ç½®å€¼æˆ–ä½¿ç”¨ç°æœ‰å€¼
        if st.session_state.clear_pressed:
            value = default_values[feature]
        else:
            value = st.session_state.get(f"pyrolysis_{feature}", default_values[feature])
        
        # è·å–è¯¥ç‰¹å¾çš„èŒƒå›´
        min_val, max_val = feature_ranges[feature]
        
        # ç®€å•çš„ä¸¤åˆ—å¸ƒå±€
        col_a, col_b = st.columns([1, 0.5])
        with col_a:
            st.markdown(f"<div class='input-label' style='background-color: #FF7F50;'>{feature}</div>", unsafe_allow_html=True)
        with col_b:
            features[feature] = st.number_input(
                "", 
                min_value=min_val, 
                max_value=max_val, 
                value=value, 
                key=f"pyrolysis_{feature}", 
                format="%.1f",
                label_visibility="collapsed"
            )

# é‡ç½®session_stateä¸­çš„clear_pressedçŠ¶æ€
if st.session_state.clear_pressed:
    st.session_state.clear_pressed = False

# è½¬æ¢ä¸ºDataFrame
input_data = pd.DataFrame([features])

# é¢„æµ‹ç»“æœæ˜¾ç¤ºåŒºåŸŸå’ŒæŒ‰é’®
result_col, button_col = st.columns([3, 1])

with result_col:
    prediction_placeholder = st.empty()
    warning_placeholder = st.empty()
    
with button_col:
    predict_button = st.button("PUSH", key="predict")
    
    # å®šä¹‰ClearæŒ‰é’®çš„å›è°ƒå‡½æ•°
    def clear_values():
        st.session_state.clear_pressed = True
        # æ¸…é™¤æ˜¾ç¤º
        if 'prediction_result' in st.session_state:
            st.session_state.prediction_result = None
        if 'warnings' in st.session_state:
            st.session_state.warnings = None
    
    clear_button = st.button("CLEAR", key="clear", on_click=clear_values)

# å¤„ç†é¢„æµ‹é€»è¾‘
if predict_button:
    try:
        # è®°å½•è¾“å…¥æ•°æ®
        log("è¿›è¡Œé¢„æµ‹:")
        log(f"è¾“å…¥æ•°æ®: {input_data.to_dict('records')}")
        
        # æ•è·è­¦å‘Šï¼ˆå¦‚æœé¢„æµ‹å™¨æœ‰æ­¤æ–¹æ³•ï¼‰
        warnings_list = []
        if hasattr(predictor, 'check_input_range'):
            warnings_list = predictor.check_input_range(input_data)
            st.session_state.warnings = warnings_list
        
        # ä½¿ç”¨predictorè¿›è¡Œé¢„æµ‹
        log("è°ƒç”¨é¢„æµ‹å™¨çš„predictæ–¹æ³•")
        y_pred = predictor.predict(input_data)[0]
        log(f"é¢„æµ‹å®Œæˆ: {y_pred:.2f}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœåˆ°session_state
        st.session_state.prediction_result = y_pred

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        prediction_placeholder.markdown(
            f"<div class='yield-result'>Char Yield (wt%) <br> {y_pred:.2f}</div>",
            unsafe_allow_html=True
        )
        
        # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        if warnings_list:
            warning_text = "<div style='color:orange;padding:10px;margin-top:10px;'><b>âš ï¸ è­¦å‘Š:</b> ä»¥ä¸‹è¾“å…¥å€¼è¶…å‡ºè®­ç»ƒèŒƒå›´ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§:<br>"
            for warning in warnings_list:
                warning_text += f"- {warning}<br>"
            warning_text += "</div>"
            warning_placeholder.markdown(warning_text, unsafe_allow_html=True)
    except Exception as e:
        log(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        log(traceback.format_exc())
        st.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

# å¦‚æœæœ‰ä¿å­˜çš„é¢„æµ‹ç»“æœï¼Œæ˜¾ç¤ºå®ƒ
if 'prediction_result' in st.session_state and st.session_state.prediction_result is not None:
    prediction_placeholder.markdown(
        f"<div class='yield-result'>Char Yield (wt%) <br> {st.session_state.prediction_result:.2f}</div>",
        unsafe_allow_html=True
    )
    
    # æ˜¾ç¤ºä¿å­˜çš„è­¦å‘Š
    if 'warnings' in st.session_state and st.session_state.warnings:
        warning_text = "<div style='color:orange;padding:10px;margin-top:10px;'><b>âš ï¸ è­¦å‘Š:</b> ä»¥ä¸‹è¾“å…¥å€¼è¶…å‡ºè®­ç»ƒèŒƒå›´ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§:<br>"
        for warning in st.session_state.warnings:
            warning_text += f"- {warning}<br>"
        warning_text += "</div>"
        warning_placeholder.markdown(warning_text, unsafe_allow_html=True)

# æ·»åŠ è°ƒè¯•ä¿¡æ¯
with st.expander("Debug Information", expanded=False):
    st.write("Input Features:")
    st.write(input_data)
    
    if predictor is not None:
        st.write("Predictor Information:")
        predictor_info = {
            "Type": type(predictor).__name__
        }
        if hasattr(predictor, 'feature_names'):
            predictor_info["Feature Names"] = predictor.feature_names
        if hasattr(predictor, 'performance'):
            predictor_info["Performance"] = predictor.performance
        st.write(predictor_info)
        
        if hasattr(predictor, 'metadata') and predictor.metadata:
            st.write("Model Metadata:")
            st.write(predictor.metadata)

# æ·»åŠ å…³äºæ¨¡å‹çš„ä¿¡æ¯
st.markdown("""
### About the Model
This application uses a CatBoost ensemble model to predict char yield in biomass pyrolysis.

#### Key Factors Affecting Char Yield:
- **Pyrolysis Temperature**: Higher temperature generally decreases char yield
- **Residence Time**: Longer residence time generally increases char yield
- **Biomass Composition**: Carbon content and ash content significantly affect the final yield

The model was trained using 10-fold cross-validation with optimized hyperparameters, achieving high prediction accuracy (RÂ² = 0.93, RMSE = 3.39 on test set).
""")